{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ce5468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Reddy\\Documents\\Projects\\posture correction\\dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the working directory to where your dataset files are located\n",
    "os.chdir('C:/Users/Reddy/Documents/Projects/posture correction/dataset')\n",
    "\n",
    "# Print the current working directory to confirm it worked\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9377a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded successfully!\n",
      "Landmarks shape: (1372, 100)\n",
      "Labels shape: (1372, 2)\n",
      "Angles shape: (1372, 8)\n",
      "3D Distances shape: (1372, 17)\n",
      "XYZ Distances shape: (1372, 49)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets using the full file path\n",
    "try:\n",
    "    df_3d_distances = pd.read_csv(\"C:/Users/Reddy/Documents/Projects/posture correction/dataset/3d_distances.csv\")\n",
    "    df_angles = pd.read_csv(\"C:/Users/Reddy/Documents/Projects/posture correction/dataset/angles.csv\")\n",
    "    df_labels = pd.read_csv(\"C:/Users/Reddy/Documents/Projects/posture correction/dataset/labels.csv\")\n",
    "    df_landmarks = pd.read_csv(\"C:/Users/Reddy/Documents/Projects/posture correction/dataset/landmarks.csv\")\n",
    "    df_xyz_distances = pd.read_csv(\"C:/Users/Reddy/Documents/Projects/posture correction/dataset/xyz_distances.csv\")\n",
    "\n",
    "    print(\"All datasets loaded successfully!\")\n",
    "    print(f\"Landmarks shape: {df_landmarks.shape}\")\n",
    "    print(f\"Labels shape: {df_labels.shape}\")\n",
    "    print(f\"Angles shape: {df_angles.shape}\")\n",
    "    print(f\"3D Distances shape: {df_3d_distances.shape}\")\n",
    "    print(f\"XYZ Distances shape: {df_xyz_distances.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: A file was not found. Please double-check the path. Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63db8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "           POSTURE CORRECTION DATASET OVERVIEW\n",
      "==================================================\n",
      "\n",
      "==================== LANDMARKS DATASET ====================\n",
      "Shape: (1372, 100)\n",
      "\n",
      "Data types:\n",
      "pose_id                             int64\n",
      "x_nose                              float64\n",
      "y_nose                              float64\n",
      "z_nose                              float64\n",
      "x_left_eye_inner                    float64\n",
      "y_left_eye_inner                    float64\n",
      "z_left_eye_inner                    float64\n",
      "x_left_eye                          float64\n",
      "y_left_eye                          float64\n",
      "z_left_eye                          float64\n",
      "x_left_eye_outer                    float64\n",
      "y_left_eye_outer                    float64\n",
      "z_left_eye_outer                    float64\n",
      "x_right_eye_inner                   float64\n",
      "y_right_eye_inner                   float64\n",
      "z_right_eye_inner                   float64\n",
      "x_right_eye                         float64\n",
      "y_right_eye                         float64\n",
      "z_right_eye                         float64\n",
      "x_right_eye_outer                   float64\n",
      "y_right_eye_outer                   float64\n",
      "z_right_eye_outer                   float64\n",
      "x_left_ear                          float64\n",
      "y_left_ear                          float64\n",
      "z_left_ear                          float64\n",
      "x_right_ear                         float64\n",
      "y_right_ear                         float64\n",
      "z_right_ear                         float64\n",
      "x_mouth_left                        float64\n",
      "y_mouth_left                        float64\n",
      "z_mouth_left                        float64\n",
      "x_mouth_right                       float64\n",
      "y_mouth_right                       float64\n",
      "z_mouth_right                       float64\n",
      "x_left_shoulder                     float64\n",
      "y_left_shoulder                     float64\n",
      "z_left_shoulder                     float64\n",
      "x_right_shoulder                    float64\n",
      "y_right_shoulder                    float64\n",
      "z_right_shoulder                    float64\n",
      "x_left_elbow                        float64\n",
      "y_left_elbow                        float64\n",
      "z_left_elbow                        float64\n",
      "x_right_elbow                       float64\n",
      "y_right_elbow                       float64\n",
      "z_right_elbow                       float64\n",
      "x_left_wrist                        float64\n",
      "y_left_wrist                        float64\n",
      "z_left_wrist                        float64\n",
      "x_right_wrist                       float64\n",
      "y_right_wrist                       float64\n",
      "z_right_wrist                       float64\n",
      "x_left_pinky_1                      float64\n",
      "y_left_pinky_1                      float64\n",
      "z_left_pinky_1                      float64\n",
      "x_right_pinky_1                     float64\n",
      "y_right_pinky_1                     float64\n",
      "z_right_pinky_1                     float64\n",
      "x_left_index_1                      float64\n",
      "y_left_index_1                      float64\n",
      "z_left_index_1                      float64\n",
      "x_right_index_1                     float64\n",
      "y_right_index_1                     float64\n",
      "z_right_index_1                     float64\n",
      "x_left_thumb_2                      float64\n",
      "y_left_thumb_2                      float64\n",
      "z_left_thumb_2                      float64\n",
      "x_right_thumb_2                     float64\n",
      "y_right_thumb_2                     float64\n",
      "z_right_thumb_2                     float64\n",
      "x_left_hip                          float64\n",
      "y_left_hip                          float64\n",
      "z_left_hip                          float64\n",
      "x_right_hip                         float64\n",
      "y_right_hip                         float64\n",
      "z_right_hip                         float64\n",
      "x_left_knee                         float64\n",
      "y_left_knee                         float64\n",
      "z_left_knee                         float64\n",
      "x_right_knee                        float64\n",
      "y_right_knee                        float64\n",
      "z_right_knee                        float64\n",
      "x_left_ankle                        float64\n",
      "y_left_ankle                        float64\n",
      "z_left_ankle                        float64\n",
      "x_right_ankle                       float64\n",
      "y_right_ankle                       float64\n",
      "z_right_ankle                       float64\n",
      "x_left_heel                         float64\n",
      "y_left_heel                         float64\n",
      "z_left_heel                         float64\n",
      "x_right_heel                        float64\n",
      "y_right_heel                        float64\n",
      "z_right_heel                        float64\n",
      "x_left_foot_index                   float64\n",
      "y_left_foot_index                   float64\n",
      "z_left_foot_index                   float64\n",
      "x_right_foot_index                  float64\n",
      "y_right_foot_index                  float64\n",
      "z_right_foot_index                  float64\n",
      "\n",
      "First 5 rows:\n",
      "   pose_id    x_nose     y_nose      z_nose  x_left_eye_inner  y_left_eye_inner  z_left_eye_inner  x_left_eye  y_left_eye  z_left_eye  x_left_eye_outer  y_left_eye_outer  z_left_eye_outer  x_right_eye_inner  y_right_eye_inner  z_right_eye_inner  x_right_eye  y_right_eye  z_right_eye  x_right_eye_outer  y_right_eye_outer  z_right_eye_outer  x_left_ear  y_left_ear  z_left_ear  x_right_ear  y_right_ear  z_right_ear  x_mouth_left  y_mouth_left  z_mouth_left  x_mouth_right  y_mouth_right  z_mouth_right  x_left_shoulder  y_left_shoulder  z_left_shoulder  x_right_shoulder  y_right_shoulder  z_right_shoulder  x_left_elbow  y_left_elbow  z_left_elbow  x_right_elbow  y_right_elbow  z_right_elbow  x_left_wrist  y_left_wrist  z_left_wrist  x_right_wrist  y_right_wrist  z_right_wrist  x_left_pinky_1  y_left_pinky_1  z_left_pinky_1  x_right_pinky_1  y_right_pinky_1  z_right_pinky_1  x_left_index_1  y_left_index_1  z_left_index_1  x_right_index_1  y_right_index_1  z_right_index_1  x_left_thumb_2  y_left_thumb_2  z_left_thumb_2  x_right_thumb_2  y_right_thumb_2  z_right_thumb_2  x_left_hip  y_left_hip  z_left_hip  x_right_hip  y_right_hip  z_right_hip  x_left_knee  y_left_knee  z_left_knee  x_right_knee  y_right_knee  z_right_knee  x_left_ankle  y_left_ankle  z_left_ankle  x_right_ankle  y_right_ankle  z_right_ankle  x_left_heel  y_left_heel  z_left_heel  x_right_heel  y_right_heel  z_right_heel  x_left_foot_index  y_left_foot_index  z_left_foot_index  x_right_foot_index  y_right_foot_index  z_right_foot_index\n",
      "0        0 -5.889507 -57.637520  -45.019750         -4.656085        -62.832863        -44.571823   -3.302626  -63.386856  -44.567863         -2.032406        -63.872166         -44.56217          -7.365534         -61.547394         -43.222190    -8.094638   -61.181343   -43.219574          -8.794889         -60.837234         -43.221565    0.320341  -63.057720  -35.625750    -8.877375   -58.755780   -29.455366     -3.112062    -54.170460    -41.242752      -6.512476     -53.172638     -39.353046         7.621102       -41.408276       -24.472181         -9.392822        -38.572105        -23.247710      9.644700    -19.461535    -11.067570     -11.833770     -16.390108     -13.978561     10.260753     -4.875073    -12.865825     -11.489133       1.088664     -19.914438       10.423702        2.853016      -14.993233       -11.643494         7.434713       -22.953787        9.608641        2.667237      -17.855345       -10.314507         6.941773       -25.846296        8.956856        0.192237      -13.945213        -9.877070         4.980656       -21.360184    4.530677   -0.769883    0.434257    -4.530680     0.769883    -0.434257     5.482031    22.849077     5.814251     -3.823916     21.405060      7.528501      5.240161     33.986900     52.543015      -1.359286      34.199524      46.699230     3.615098    27.651157    56.852562     -0.842025     35.037060     50.565020           5.842190          45.971020          50.263714            0.092779           45.842150           41.427795\n",
      "1        1 -4.255504 -62.935925 -128.907500         -2.977403        -67.035990       -124.258545   -2.215265  -67.198250 -124.263240         -1.494903        -67.271600        -124.28937          -5.490019         -66.694050        -123.841034    -6.242488   -66.595990  -123.868520          -6.970745         -66.471275        -123.887270    0.158871  -66.276910  -94.840770    -7.382975   -65.178630   -93.389630     -2.405174    -59.359146   -117.840805      -5.484713     -58.614414    -117.411385         9.621441       -39.551704       -76.575966        -13.079389        -40.373524        -69.610660     13.726971     -9.250046    -83.728780     -18.751081     -10.955673     -72.177956     15.243090     19.005112   -105.679085     -22.634450      19.847279     -94.470210       16.548235       28.064500     -114.315220       -24.552680        27.681068      -101.940940       15.545763       28.099240     -118.723820       -23.852718        27.994123      -108.408020       14.187283       30.526299     -107.923040       -22.900806        24.882238       -97.551476    6.083795    0.082059   -1.683104    -6.083787    -0.082059     1.683104     3.493807    21.059874   -99.830010     -7.086227     21.279276    -92.234070      4.397081     52.501846    -20.234724      -3.125374      51.500397     -13.009045     4.149046    56.273407   -14.129170     -1.298891     54.733307     -6.886051           3.980098          65.370830         -49.023930           -5.090634           65.641780          -42.878056\n",
      "2        2 -2.878917 -61.709988 -137.453340         -1.619050        -65.693750       -132.181660   -0.785822  -65.814340 -132.184070         -0.019743        -65.890350        -132.21225          -4.078131         -65.586700        -132.124880    -4.935132   -65.607765  -132.152440          -5.671054         -65.631760        -132.173930    1.549772  -65.182160  -99.453590    -6.610276   -64.336230   -99.910890     -1.306075    -58.035330   -125.136986      -4.306733     -57.604250    -125.276115         9.978933       -39.912310       -78.791150        -13.220856        -40.021996        -73.317760     16.538727     -5.446848    -86.096214     -19.945402     -10.250859     -78.950134     18.549637     22.050380   -109.449210     -24.251736      20.703995    -104.966140       20.340712       31.081423     -118.517914       -27.143755        28.058228      -113.046010       18.187770       30.579784     -123.642456       -26.358335        28.842712      -119.784740       17.251207       28.861786     -112.006820       -24.714437        25.727966      -108.225464    6.351656   -0.002397   -1.533348    -6.351656     0.002380     1.533348     4.476383    17.047047  -112.183250     -7.616309     17.310991   -104.356830      4.062827     49.782406    -26.564451      -3.859388      48.021600     -21.946623     3.412772    53.354908   -19.904400     -2.119770     51.265694    -15.554097           1.994894          62.725025         -57.717957           -4.452602           62.494457          -53.804527\n",
      "3        3 -4.242575 -60.371220 -135.094830         -3.118133        -64.416000       -129.995930   -2.369744  -64.603290 -130.003400         -1.753780        -64.713486        -130.03014          -5.249705         -63.894188        -129.775020    -5.842634   -63.733406  -129.804730          -6.406885         -63.516680        -129.817890   -0.396844  -63.734180  -99.674950    -6.678256   -62.118004   -98.685520     -2.542080    -57.574074   -123.973830      -5.185241     -56.916283    -123.659720         8.351439       -41.103490       -78.614876        -11.962355        -38.814983        -70.153770     16.352345    -19.105413    -80.316800     -19.314741     -13.103511     -68.429740     20.802422      5.574176    -99.007770     -28.422815      11.460391     -88.124130       22.555964       12.941505     -105.348100       -31.474527        17.327095       -93.918660       21.757057       13.627078     -111.581650       -31.827667        17.337917      -101.712100       20.899534       11.227438     -101.930890       -30.599478        15.769301       -91.864580    5.397311   -0.089862   -2.822852    -5.397311     0.089862     2.822852     3.044350    27.221813   -68.492000     -5.401288     27.346409    -58.877575      2.724179     57.130005    -10.227795      -2.849652      57.269690      -2.069154     2.699142    59.994877    -6.855729     -1.485475     59.729427      1.433403           1.950102          68.187256         -42.989098           -4.573338           68.144350          -34.117043\n",
      "4        4 -0.805543 -56.178570  -41.124413         -0.055174        -58.501305        -37.938560    0.456936  -58.473960  -37.954430          0.969290        -58.420975         -37.96570          -1.600344         -58.466236         -37.740726    -2.109919   -58.391834   -37.741910          -2.580076         -58.267300         -37.747080    1.721020  -56.863434  -19.394129    -3.009482   -56.525833   -18.784786      0.342620    -53.399857    -34.320892      -1.682060     -53.466946     -34.150360         6.614853       -40.665200        -9.521072         -6.836676        -39.334476         -6.535229     11.046882    -24.763676     -3.101772     -10.401074     -21.691212     -10.795914     15.967958    -13.095023    -19.778528     -15.010607     -12.113915     -39.956780       18.233288       -9.470372      -24.322199       -17.193197        -8.687159       -46.510430       18.118427       -9.990546      -29.795815       -17.335106        -8.800731       -51.465015       17.112183      -10.936589      -22.393103       -16.284739        -9.693833       -43.066036    3.732736   -0.207420   -0.864446    -3.732722     0.207434     0.864446     2.875056    25.615683     5.187430     -3.736604     27.157059      6.090599      2.415785     49.650090     44.187866      -2.654587      50.472115      43.241386     1.911180    51.426052    47.124107     -2.455719     52.861732     45.936783           2.699764          57.254112          27.531416           -2.288348           57.803005           26.288315\n",
      "\n",
      "Null values: None\n",
      "\n",
      "Memory usage: 1.05 MB\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "==================== LABELS DATASET ====================\n",
      "Shape: (1372, 2)\n",
      "\n",
      "Data types:\n",
      "pose_id                             int64\n",
      "pose                                object\n",
      "\n",
      "First 5 rows:\n",
      "   pose_id                pose\n",
      "0        0  jumping_jacks_down\n",
      "1        1  jumping_jacks_down\n",
      "2        2  jumping_jacks_down\n",
      "3        3  jumping_jacks_down\n",
      "4        4  jumping_jacks_down\n",
      "\n",
      "Null values: None\n",
      "\n",
      "Memory usage: 0.09 MB\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "==================== ANGLES DATASET ====================\n",
      "Shape: (1372, 8)\n",
      "\n",
      "Data types:\n",
      "pose_id                             int64\n",
      "right_elbow_right_shoulder_right_hip float64\n",
      "left_elbow_left_shoulder_left_hip   float64\n",
      "right_knee_mid_hip_left_knee        float64\n",
      "right_hip_right_knee_right_ankle    float64\n",
      "left_hip_left_knee_left_ankle       float64\n",
      "right_wrist_right_elbow_right_shoulder float64\n",
      "left_wrist_left_elbow_left_shoulder float64\n",
      "\n",
      "First 5 rows:\n",
      "   pose_id  right_elbow_right_shoulder_right_hip  left_elbow_left_shoulder_left_hip  right_knee_mid_hip_left_knee  right_hip_right_knee_right_ankle  left_hip_left_knee_left_ankle  right_wrist_right_elbow_right_shoulder  left_wrist_left_elbow_left_shoulder\n",
      "0        0                             14.021934                           8.209491                     23.214672                        129.223340                     116.203650                              138.059770                            141.57338\n",
      "1        1                             67.077340                          75.857120                      6.334966                         33.741722                      33.625260                              149.027540                            155.01839\n",
      "2        2                             74.106384                          75.378140                      6.424555                         29.761814                      29.707142                              150.370900                            150.90870\n",
      "3        3                             60.998230                          68.259796                      7.821799                         51.646942                      49.793900                              139.172230                            146.24620\n",
      "4        4                             28.631880                          20.923730                     13.974962                        133.069700                     134.827160                              122.640205                            105.69104\n",
      "\n",
      "Null values: None\n",
      "\n",
      "Memory usage: 0.08 MB\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "==================== 3D DISTANCES DATASET ====================\n",
      "Shape: (1372, 17)\n",
      "\n",
      "Data types:\n",
      "pose_id                             int64\n",
      "left_shoulder_left_wrist            float64\n",
      "right_shoulder_right_wrist          float64\n",
      "left_hip_left_ankle                 float64\n",
      "right_hip_right_ankle               float64\n",
      "left_hip_left_wrist                 float64\n",
      "right_hip_right_wrist               float64\n",
      "left_shoulder_left_ankle            float64\n",
      "right_shoulder_right_ankle          float64\n",
      "left_hip_right_wrist                float64\n",
      "right_hip_left_wrist                float64\n",
      "left_elbow_right_elbow              float64\n",
      "left_knee_right_knee                float64\n",
      "left_wrist_right_wrist              float64\n",
      "left_ankle_right_ankle              float64\n",
      "left_hip_avg_left_wrist_left_ankle  float64\n",
      "right_hip_avg_right_wrist_right_ankle float64\n",
      "\n",
      "First 5 rows:\n",
      "   pose_id  left_shoulder_left_wrist  right_shoulder_right_wrist  left_hip_left_ankle  right_hip_right_ankle  left_hip_left_wrist  right_hip_right_wrist  left_shoulder_left_ankle  right_shoulder_right_ankle  left_hip_right_wrist  right_hip_left_wrist  left_elbow_right_elbow  left_knee_right_knee  left_wrist_right_wrist  left_ankle_right_ankle  left_hip_avg_left_wrist_left_ankle  right_hip_avg_right_wrist_right_ankle\n",
      "0        0                 38.423300                   39.855762            62.640720              57.871964            15.052525              20.688140                 107.80279                  101.256220             25.964552             20.129477               21.891373              9.572068               23.628513                8.817467                           24.935423                              21.897507\n",
      "1        1                 65.631510                   65.847110            55.631300              53.715553           106.099660              99.581940                 108.05316                  108.368080             99.120370            111.111630               34.513107             13.026263               39.510204               10.478581                           70.999214                              66.304980\n",
      "2        2                 69.661674                   69.360990            55.770256              53.510430           110.819440             109.959590                 103.96043                  102.363594            109.834720            115.859040               37.486490             14.406796               43.056570                9.337358                           75.719420                              73.916530\n",
      "3        3                 52.437600                   55.870330            57.758915              57.445232            97.575290              94.503000                 119.82603                  118.113594             92.485230            105.289980               38.071854             12.797712               50.756527                9.881827                           60.924847                              59.798840\n",
      "4        4                 30.867628                   43.872227            67.210240              65.753365            25.952522              44.106445                 105.16242                  102.763930             44.958740             31.483402               22.992490              6.848766               36.983727                5.223047                           23.287280                              19.660568\n",
      "\n",
      "Null values: None\n",
      "\n",
      "Memory usage: 0.18 MB\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "==================== XYZ DISTANCES DATASET ====================\n",
      "Shape: (1372, 49)\n",
      "\n",
      "Data types:\n",
      "pose_id                             int64\n",
      "x_left_shoulder_left_wrist          float64\n",
      "y_left_shoulder_left_wrist          float64\n",
      "z_left_shoulder_left_wrist          float64\n",
      "x_right_shoulder_right_wrist        float64\n",
      "y_right_shoulder_right_wrist        float64\n",
      "z_right_shoulder_right_wrist        float64\n",
      "x_left_hip_left_ankle               float64\n",
      "y_left_hip_left_ankle               float64\n",
      "z_left_hip_left_ankle               float64\n",
      "x_right_hip_right_ankle             float64\n",
      "y_right_hip_right_ankle             float64\n",
      "z_right_hip_right_ankle             float64\n",
      "x_left_hip_left_wrist               float64\n",
      "y_left_hip_left_wrist               float64\n",
      "z_left_hip_left_wrist               float64\n",
      "x_right_hip_right_wrist             float64\n",
      "y_right_hip_right_wrist             float64\n",
      "z_right_hip_right_wrist             float64\n",
      "x_left_shoulder_left_ankle          float64\n",
      "y_left_shoulder_left_ankle          float64\n",
      "z_left_shoulder_left_ankle          float64\n",
      "x_right_shoulder_right_ankle        float64\n",
      "y_right_shoulder_right_ankle        float64\n",
      "z_right_shoulder_right_ankle        float64\n",
      "x_left_hip_right_wrist              float64\n",
      "y_left_hip_right_wrist              float64\n",
      "z_left_hip_right_wrist              float64\n",
      "x_right_hip_left_wrist              float64\n",
      "y_right_hip_left_wrist              float64\n",
      "z_right_hip_left_wrist              float64\n",
      "x_left_elbow_right_elbow            float64\n",
      "y_left_elbow_right_elbow            float64\n",
      "z_left_elbow_right_elbow            float64\n",
      "x_left_knee_right_knee              float64\n",
      "y_left_knee_right_knee              float64\n",
      "z_left_knee_right_knee              float64\n",
      "x_left_wrist_right_wrist            float64\n",
      "y_left_wrist_right_wrist            float64\n",
      "z_left_wrist_right_wrist            float64\n",
      "x_left_ankle_right_ankle            float64\n",
      "y_left_ankle_right_ankle            float64\n",
      "z_left_ankle_right_ankle            float64\n",
      "x_left_hip_avg_left_wrist_left_ankle float64\n",
      "y_left_hip_avg_left_wrist_left_ankle float64\n",
      "z_left_hip_avg_left_wrist_left_ankle float64\n",
      "x_right_hip_avg_right_wrist_right_ankle float64\n",
      "y_right_hip_avg_right_wrist_right_ankle float64\n",
      "z_right_hip_avg_right_wrist_right_ankle float64\n",
      "\n",
      "First 5 rows:\n",
      "   pose_id  x_left_shoulder_left_wrist  y_left_shoulder_left_wrist  z_left_shoulder_left_wrist  x_right_shoulder_right_wrist  y_right_shoulder_right_wrist  z_right_shoulder_right_wrist  x_left_hip_left_ankle  y_left_hip_left_ankle  z_left_hip_left_ankle  x_right_hip_right_ankle  y_right_hip_right_ankle  z_right_hip_right_ankle  x_left_hip_left_wrist  y_left_hip_left_wrist  z_left_hip_left_wrist  x_right_hip_right_wrist  y_right_hip_right_wrist  z_right_hip_right_wrist  x_left_shoulder_left_ankle  y_left_shoulder_left_ankle  z_left_shoulder_left_ankle  x_right_shoulder_right_ankle  y_right_shoulder_right_ankle  z_right_shoulder_right_ankle  x_left_hip_right_wrist  y_left_hip_right_wrist  z_left_hip_right_wrist  x_right_hip_left_wrist  y_right_hip_left_wrist  z_right_hip_left_wrist  x_left_elbow_right_elbow  y_left_elbow_right_elbow  z_left_elbow_right_elbow  x_left_knee_right_knee  y_left_knee_right_knee  z_left_knee_right_knee  x_left_wrist_right_wrist  y_left_wrist_right_wrist  z_left_wrist_right_wrist  x_left_ankle_right_ankle  y_left_ankle_right_ankle  z_left_ankle_right_ankle  x_left_hip_avg_left_wrist_left_ankle  y_left_hip_avg_left_wrist_left_ankle  z_left_hip_avg_left_wrist_left_ankle  x_right_hip_avg_right_wrist_right_ankle  y_right_hip_avg_right_wrist_right_ankle  z_right_hip_avg_right_wrist_right_ankle\n",
      "0        0                    2.639651                   36.533203                   11.606357                     -2.096311                     39.660770                      3.333271               0.709485              34.756783              52.108757                 3.171394                33.429640                47.133488               5.730076              -4.105191             -13.300081                -6.958453                 0.318782               -19.480180                   -2.380940                    75.39517                   77.015200                      8.033537                     72.771630                     69.946940              -16.019810                1.858547              -20.348696               14.791432               -5.644956              -12.431568                -21.478470                  3.071426                 -2.910992               -9.305947               -1.444017                1.714250                -21.749886                  5.963738                 -7.048614                 -6.599447                  0.212624                 -5.843784                             -3.219780                            -15.325796                            -19.404337                                 1.893529                               -16.874212                               -13.826653\n",
      "1        1                    5.621649                   58.556816                  -29.103119                     -9.555062                     60.220802                    -24.859550              -1.686714              52.419790             -18.551620                 2.958412                51.582455               -14.692148               9.159294              18.923054            -103.995980               -16.550665                19.929337               -96.153310                   -5.224360                    92.05355                   56.341240                      9.954015                     91.873920                     56.601612              -28.718246               19.765220              -92.787100               21.326876               19.087170             -107.362190                -32.478050                 -1.705627                 11.550827              -10.580033                0.219402                7.595940                -37.877540                  0.842167                 11.208878                 -7.522455                 -1.001450                  7.225679                             -3.736290                            -35.671420                             61.273800                                 6.796126                               -35.755894                                55.422730\n",
      "2        2                    8.570704                   61.962692                  -30.658058                    -11.030880                     60.725990                    -31.648384              -2.288830              49.784800             -25.031103                 2.492269                48.019220               -23.479970              12.197981              22.052778            -107.915860               -17.900080                20.701614              -106.499490                   -5.916107                    89.69472                   52.226700                      9.361468                     88.043594                     51.371130              -30.603392               20.706392             -103.432790               24.901293               22.048000             -110.982560                -36.484130                 -4.804011                  7.146080              -12.092692                0.263945                7.826424                -42.801373                 -1.346386                  4.483070                 -7.922214                 -1.760807                  4.617828                             -4.954575                            -35.918790                             66.473480                                 7.703906                               -34.360416                                64.989730\n",
      "3        3                   12.450983                   46.677666                  -20.392890                    -16.460460                     50.275375                    -17.970360              -2.673132              57.219868              -7.404943                 2.547658                57.179830                -4.892006              15.405111               5.664037             -96.184910               -23.025505                11.370529               -90.946980                   -5.627260                    98.23349                   68.387085                      9.112702                     96.084670                     68.084620              -33.820126               11.550253              -85.301280               26.199732                5.484314             -101.830620                -35.667084                  6.001903                 11.887062               -8.445639                0.124596                9.614422                -49.225235                  5.886215                 10.883636                 -5.573831                  0.139687                  8.158640                             -6.365989                            -31.441952                             51.794930                                10.238924                               -34.275177                                47.919495\n",
      "4        4                    9.353104                   27.570175                  -10.257456                     -8.173931                     27.220562                    -33.421550              -1.316952              49.857510              45.052310                 1.078135                50.264680                42.376940              12.235221             -12.887604             -18.914082               -11.277885               -12.321349               -40.821224                   -4.199069                    90.31529                   53.708940                      4.182089                     89.806595                     49.776615              -18.743343              -11.906496              -39.092335               19.700680              -13.302457              -20.642975                -21.447956                  3.072464                 -7.694142               -6.611660                1.541376                0.903169                -30.978565                  0.981108                -20.178251                 -5.070372                  0.822025                 -0.946480                             -5.459134                            -18.484953                            -13.069115                                 5.099875                               -18.971666                                -0.777858\n",
      "\n",
      "Null values: None\n",
      "\n",
      "Memory usage: 0.51 MB\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============== DATASET OVERVIEW ==============\n",
    "print(\"=\"*50)\n",
    "print(\"           POSTURE CORRECTION DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def display_dataset_overview(df, dataset_name):\n",
    "    \"\"\"Display comprehensive overview of a dataset\"\"\"\n",
    "    print(f\"\\n{'='*20} {dataset_name} {'='*20}\")\n",
    "    \n",
    "    # Dataset shape\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"{col:<35} {dtype}\")\n",
    "    \n",
    "    # First 5 rows\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head().to_string())\n",
    "    \n",
    "    # Null values\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        print(f\"\\nNull values:\")\n",
    "        for col, count in null_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"{col:<35} {count}\")\n",
    "    else:\n",
    "        print(f\"\\nNull values: None\")\n",
    "    \n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Display overview for each dataset\n",
    "datasets = [\n",
    "    (df_landmarks, \"LANDMARKS DATASET\"),\n",
    "    (df_labels, \"LABELS DATASET\"), \n",
    "    (df_angles, \"ANGLES DATASET\"),\n",
    "    (df_3d_distances, \"3D DISTANCES DATASET\"),\n",
    "    (df_xyz_distances, \"XYZ DISTANCES DATASET\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    display_dataset_overview(df, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003b1382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "           DATASET RELATIONSHIPS & MERGING\n",
      "==================================================\n",
      "\n",
      "Dataset Sizes:\n",
      "Landmarks:     1372 rows\n",
      "Labels:        1372 rows\n",
      "Angles:        1372 rows\n",
      "3D Distances:  1372 rows\n",
      "XYZ Distances: 1372 rows\n",
      "\n",
      "Pose ID Ranges:\n",
      "Landmarks      : 0 to 1371 (1372 unique IDs)\n",
      "Labels         : 0 to 1371 (1372 unique IDs)\n",
      "Angles         : 0 to 1371 (1372 unique IDs)\n",
      "3D Distances   : 0 to 1371 (1372 unique IDs)\n",
      "XYZ Distances  : 0 to 1371 (1372 unique IDs)\n",
      "\n",
      "Class Distribution:\n",
      "jumping_jacks_down       :  189 samples ( 13.8%)\n",
      "jumping_jacks_up         :  181 samples ( 13.2%)\n",
      "pullups_down             :  154 samples ( 11.2%)\n",
      "pushups_up               :  144 samples ( 10.5%)\n",
      "squats_up                :  139 samples ( 10.1%)\n",
      "pullups_up               :  135 samples (  9.8%)\n",
      "squats_down              :  127 samples (  9.3%)\n",
      "pushups_down             :  102 samples (  7.4%)\n",
      "situp_down               :  102 samples (  7.4%)\n",
      "situp_up                 :   99 samples (  7.2%)\n",
      "\n",
      "Merging all datasets on 'pose_id'...\n",
      "Final merged dataset shape: (1372, 172)\n",
      "Total features after merging: 171 (excluding pose_id)\n",
      "\n",
      "Feature Breakdown:\n",
      "Landmark coordinates: 99 features\n",
      "Joint angles:         7 features\n",
      "3D distances:         16 features\n",
      "XYZ distances:        48 features\n",
      "Target labels:        1 feature (pose)\n",
      "Total:                171 features\n"
     ]
    }
   ],
   "source": [
    "# ============== DATASET RELATIONSHIPS ==============\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           DATASET RELATIONSHIPS & MERGING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if all datasets have the same number of rows and pose_id alignment\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"Landmarks:     {len(df_landmarks)} rows\")\n",
    "print(f\"Labels:        {len(df_labels)} rows\") \n",
    "print(f\"Angles:        {len(df_angles)} rows\")\n",
    "print(f\"3D Distances:  {len(df_3d_distances)} rows\")\n",
    "print(f\"XYZ Distances: {len(df_xyz_distances)} rows\")\n",
    "\n",
    "# Check pose_id ranges\n",
    "print(f\"\\nPose ID Ranges:\")\n",
    "for df, name in [(df_landmarks, \"Landmarks\"), (df_labels, \"Labels\"), (df_angles, \"Angles\"), \n",
    "                 (df_3d_distances, \"3D Distances\"), (df_xyz_distances, \"XYZ Distances\")]:\n",
    "    pose_ids = df['pose_id'].unique()\n",
    "    print(f\"{name:<15}: {min(pose_ids)} to {max(pose_ids)} ({len(pose_ids)} unique IDs)\")\n",
    "\n",
    "# Show class distribution\n",
    "print(f\"\\nClass Distribution:\")\n",
    "class_counts = df_labels['pose'].value_counts()\n",
    "for pose_class, count in class_counts.items():\n",
    "    percentage = (count / len(df_labels)) * 100\n",
    "    print(f\"{pose_class:<25}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "# Demonstrate merging all datasets\n",
    "print(f\"\\nMerging all datasets on 'pose_id'...\")\n",
    "merged_df = df_landmarks.merge(df_labels, on='pose_id') \\\n",
    "                        .merge(df_angles, on='pose_id') \\\n",
    "                        .merge(df_3d_distances, on='pose_id') \\\n",
    "                        .merge(df_xyz_distances, on='pose_id')\n",
    "\n",
    "print(f\"Final merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"Total features after merging: {merged_df.shape[1] - 1} (excluding pose_id)\")\n",
    "\n",
    "# Show column breakdown\n",
    "landmarks_cols = len([col for col in df_landmarks.columns if col != 'pose_id'])\n",
    "angles_cols = len([col for col in df_angles.columns if col != 'pose_id'])\n",
    "distances_3d_cols = len([col for col in df_3d_distances.columns if col != 'pose_id'])\n",
    "distances_xyz_cols = len([col for col in df_xyz_distances.columns if col != 'pose_id'])\n",
    "\n",
    "print(f\"\\nFeature Breakdown:\")\n",
    "print(f\"Landmark coordinates: {landmarks_cols} features\")\n",
    "print(f\"Joint angles:         {angles_cols} features\") \n",
    "print(f\"3D distances:         {distances_3d_cols} features\")\n",
    "print(f\"XYZ distances:        {distances_xyz_cols} features\")\n",
    "print(f\"Target labels:        1 feature (pose)\")\n",
    "print(f\"Total:                {landmarks_cols + angles_cols + distances_3d_cols + distances_xyz_cols + 1} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5e9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== VISUALIZATION SETUP ==============\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better looking plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"           STATISTICAL VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set up the merged dataset for comprehensive analysis\n",
    "if 'merged_df' not in locals():\n",
    "    merged_df = df_landmarks.merge(df_labels, on='pose_id') \\\n",
    "                            .merge(df_angles, on='pose_id') \\\n",
    "                            .merge(df_3d_distances, on='pose_id') \\\n",
    "                            .merge(df_xyz_distances, on='pose_id')\n",
    "\n",
    "print(f\"Visualization dataset ready: {merged_df.shape}\")\n",
    "print(\"Starting comprehensive statistical visualization analysis...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 1. CLASS DISTRIBUTION ANALYSIS ==============\n",
    "print(\"\\n1. POSE CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create subplot for class distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Pose Classification Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1.1 Bar chart of class counts\n",
    "class_counts = df_labels['pose'].value_counts()\n",
    "axes[0,0].bar(range(len(class_counts)), class_counts.values, color=sns.color_palette(\"husl\", len(class_counts)))\n",
    "axes[0,0].set_title('Pose Class Distribution (Counts)', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Pose Classes')\n",
    "axes[0,0].set_ylabel('Number of Samples')\n",
    "axes[0,0].set_xticks(range(len(class_counts)))\n",
    "axes[0,0].set_xticklabels(class_counts.index, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0,0].text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 1.2 Pie chart of class proportions\n",
    "colors = sns.color_palette(\"husl\", len(class_counts))\n",
    "axes[0,1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', \n",
    "              colors=colors, startangle=90)\n",
    "axes[0,1].set_title('Pose Class Distribution (Proportions)', fontweight='bold')\n",
    "\n",
    "# 1.3 Class balance analysis\n",
    "axes[1,0].barh(range(len(class_counts)), class_counts.values, color=colors)\n",
    "axes[1,0].set_title('Class Balance Analysis', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Sample Count')\n",
    "axes[1,0].set_ylabel('Pose Classes')\n",
    "axes[1,0].set_yticks(range(len(class_counts)))\n",
    "axes[1,0].set_yticklabels(class_counts.index)\n",
    "\n",
    "# Add percentage labels\n",
    "total_samples = len(df_labels)\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    percentage = (v / total_samples) * 100\n",
    "    axes[1,0].text(v + 10, i, f'{v} ({percentage:.1f}%)', va='center', fontweight='bold')\n",
    "\n",
    "# 1.4 Sample distribution statistics\n",
    "stats_text = f\"\"\"Dataset Statistics:\n",
    "• Total Samples: {total_samples:,}\n",
    "• Number of Classes: {len(class_counts)}\n",
    "• Most Common: {class_counts.index[0]} ({class_counts.iloc[0]} samples)\n",
    "• Least Common: {class_counts.index[-1]} ({class_counts.iloc[-1]} samples)\n",
    "• Balance Ratio: {class_counts.iloc[0]/class_counts.iloc[-1]:.2f}:1\n",
    "• Std Deviation: {class_counts.std():.1f}\"\"\"\n",
    "\n",
    "axes[1,1].text(0.1, 0.7, stats_text, transform=axes[1,1].transAxes, fontsize=11,\n",
    "               verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "axes[1,1].set_title('Dataset Statistics', fontweight='bold')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nClass Distribution Summary:\")\n",
    "for pose_class, count in class_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"{pose_class:<25}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass Balance Assessment:\")\n",
    "balance_ratio = class_counts.max() / class_counts.min()\n",
    "if balance_ratio <= 1.5:\n",
    "    print(\"✓ Well-balanced dataset\")\n",
    "elif balance_ratio <= 3.0:\n",
    "    print(\"⚠ Moderately imbalanced dataset\")\n",
    "else:\n",
    "    print(\"⚠ Highly imbalanced dataset - consider balancing techniques\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 2. FEATURE DISTRIBUTION ANALYSIS ==============\n",
    "print(\"\\n2. FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Select sample features from each dataset type for analysis\n",
    "landmark_features = [col for col in df_landmarks.columns if col != 'pose_id'][:6]\n",
    "angle_features = [col for col in df_angles.columns if col != 'pose_id'][:4]\n",
    "distance_3d_features = [col for col in df_3d_distances.columns if col != 'pose_id'][:4]\n",
    "distance_xyz_features = [col for col in df_xyz_distances.columns if col != 'pose_id'][:6]\n",
    "\n",
    "# 2.1 Landmark Coordinates Distribution\n",
    "print(\"\\n2.1 Landmark Coordinates Distribution\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Landmark Coordinates Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, feature in enumerate(landmark_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    \n",
    "    # Histogram\n",
    "    axes[row, col].hist(merged_df[feature], bins=30, alpha=0.7, color=sns.color_palette(\"husl\")[i], edgecolor='black')\n",
    "    axes[row, col].set_title(f'{feature}', fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Value')\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics text\n",
    "    mean_val = merged_df[feature].mean()\n",
    "    std_val = merged_df[feature].std()\n",
    "    axes[row, col].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[row, col].axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1, label=f'±1 STD')\n",
    "    axes[row, col].axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1)\n",
    "    axes[row, col].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2.2 Joint Angles Distribution\n",
    "print(\"\\n2.2 Joint Angles Distribution\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Joint Angles Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, feature in enumerate(angle_features):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    # Box plot with violin plot overlay\n",
    "    sns.violinplot(data=merged_df, y=feature, ax=axes[row, col], color=sns.color_palette(\"husl\")[i])\n",
    "    sns.boxplot(data=merged_df, y=feature, ax=axes[row, col], width=0.3, color='white')\n",
    "    \n",
    "    axes[row, col].set_title(f'{feature}', fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Angle (degrees)')\n",
    "    \n",
    "    # Add statistics\n",
    "    q1 = merged_df[feature].quantile(0.25)\n",
    "    q3 = merged_df[feature].quantile(0.75)\n",
    "    median = merged_df[feature].median()\n",
    "    axes[row, col].text(0.02, 0.98, f'Median: {median:.1f}°\\nIQR: {q3-q1:.1f}°', \n",
    "                       transform=axes[row, col].transAxes, fontsize=9, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2.3 Distance Features Distribution\n",
    "print(\"\\n2.3 Distance Features Distribution\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('3D Distance Features Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, feature in enumerate(distance_3d_features):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    # Histogram with KDE overlay\n",
    "    axes[row, col].hist(merged_df[feature], bins=25, alpha=0.6, color=sns.color_palette(\"husl\")[i], density=True, edgecolor='black')\n",
    "    \n",
    "    # KDE overlay\n",
    "    from scipy import stats\n",
    "    x_vals = np.linspace(merged_df[feature].min(), merged_df[feature].max(), 100)\n",
    "    kde = stats.gaussian_kde(merged_df[feature])\n",
    "    axes[row, col].plot(x_vals, kde(x_vals), 'r-', linewidth=2, label='KDE')\n",
    "    \n",
    "    axes[row, col].set_title(f'{feature}', fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Distance')\n",
    "    axes[row, col].set_ylabel('Density')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFeature Distribution Summary:\")\n",
    "print(f\"• Landmark features show varying distributions based on body part positions\")\n",
    "print(f\"• Joint angles typically range from 0° to 180° with pose-specific patterns\")\n",
    "print(f\"• Distance features show right-skewed distributions (common in distance metrics)\")\n",
    "print(f\"• Most features show continuous distributions suitable for ML algorithms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d82f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 3. CORRELATION ANALYSIS ==============\n",
    "print(\"\\n3. CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3.1 Angle Features Correlation\n",
    "print(\"\\n3.1 Joint Angles Correlation Matrix\")\n",
    "angle_cols = [col for col in df_angles.columns if col != 'pose_id']\n",
    "angle_corr = merged_df[angle_cols].corr()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Correlation heatmap\n",
    "sns.heatmap(angle_corr, annot=True, cmap='RdBu_r', center=0, \n",
    "            square=True, ax=axes[0], cbar_kws={'label': 'Correlation Coefficient'})\n",
    "axes[0].set_title('Joint Angles Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel('Joint Angles')\n",
    "axes[0].set_ylabel('Joint Angles')\n",
    "\n",
    "# Correlation strength distribution\n",
    "corr_values = angle_corr.values[np.triu_indices_from(angle_corr.values, k=1)]\n",
    "axes[1].hist(corr_values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Correlation Coefficients', fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlabel('Correlation Coefficient')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(corr_values.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {corr_values.mean():.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.2 Distance Features Correlation Sample\n",
    "print(\"\\n3.2 Distance Features Correlation Sample\")\n",
    "distance_sample_cols = distance_3d_features + distance_xyz_features[:4]\n",
    "distance_corr = merged_df[distance_sample_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(distance_corr, dtype=bool))\n",
    "sns.heatmap(distance_corr, mask=mask, annot=True, cmap='viridis', center=0,\n",
    "            square=True, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Distance Features Correlation Matrix (Sample)', fontweight='bold', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.3 Cross-feature type correlation analysis\n",
    "print(\"\\n3.3 Cross-Feature Type Correlation\")\n",
    "# Select representative features from each type\n",
    "sample_features = {\n",
    "    'Angles': angle_features[:3],\n",
    "    'Landmarks': landmark_features[:3], \n",
    "    'Distances': distance_3d_features[:3]\n",
    "}\n",
    "\n",
    "# Create cross-correlation matrix\n",
    "all_sample_features = []\n",
    "feature_labels = []\n",
    "for feat_type, features in sample_features.items():\n",
    "    all_sample_features.extend(features)\n",
    "    feature_labels.extend([f\"{feat_type}_{i+1}\" for i in range(len(features))])\n",
    "\n",
    "cross_corr = merged_df[all_sample_features].corr()\n",
    "cross_corr.index = feature_labels\n",
    "cross_corr.columns = feature_labels\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cross_corr, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Cross-Feature Type Correlation Analysis', fontweight='bold', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation insights\n",
    "print(f\"\\nCorrelation Analysis Summary:\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(angle_corr.columns)):\n",
    "    for j in range(i+1, len(angle_corr.columns)):\n",
    "        corr_val = angle_corr.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((angle_corr.columns[i], angle_corr.columns[j], corr_val))\n",
    "\n",
    "print(f\"• High correlations (|r| > 0.7) found in angle features: {len(high_corr_pairs)} pairs\")\n",
    "print(f\"• Average correlation among angle features: {corr_values.mean():.3f}\")\n",
    "print(f\"• This suggests some redundancy in joint angle measurements\")\n",
    "print(f\"• Feature selection or PCA might be beneficial for dimensionality reduction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 4. DATASET COMPARISON ANALYSIS ==============\n",
    "print(\"\\n4. DATASET COMPARISON ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4.1 Feature Range Comparison Across Datasets\n",
    "print(\"\\n4.1 Feature Value Ranges by Dataset Type\")\n",
    "\n",
    "# Prepare data for comparison\n",
    "datasets_info = [\n",
    "    ('Landmarks', df_landmarks, 'Coordinate Values'),\n",
    "    ('Angles', df_angles, 'Degrees'), \n",
    "    ('3D Distances', df_3d_distances, 'Distance Units'),\n",
    "    ('XYZ Distances', df_xyz_distances, 'Distance Units')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Feature Value Ranges Comparison Across Datasets', fontsize=16, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, df, unit) in enumerate(datasets_info):\n",
    "    # Get numerical columns only\n",
    "    numerical_cols = [col for col in df.columns if col != 'pose_id']\n",
    "    \n",
    "    # Calculate statistics for all features\n",
    "    all_values = []\n",
    "    for col in numerical_cols[:10]:  # Limit to first 10 features for visibility\n",
    "        all_values.extend(df[col].values)\n",
    "    \n",
    "    # Box plot of all feature values\n",
    "    axes[idx].boxplot([df[col].values for col in numerical_cols[:10]], \n",
    "                     labels=[col.split('_')[-1][:8] if '_' in col else col[:8] for col in numerical_cols[:10]])\n",
    "    axes[idx].set_title(f'{name} Features ({unit})', fontweight='bold')\n",
    "    axes[idx].set_ylabel(f'Values ({unit})')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add statistics\n",
    "    overall_mean = np.mean(all_values)\n",
    "    overall_std = np.std(all_values)\n",
    "    axes[idx].text(0.02, 0.98, f'Mean: {overall_mean:.2f}\\nStd: {overall_std:.2f}', \n",
    "                  transform=axes[idx].transAxes, fontsize=9, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.2 Feature Scale Comparison\n",
    "print(\"\\n4.2 Feature Scale Distribution\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Feature Scale Distribution Across Dataset Types', fontsize=16, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "scale_data = []\n",
    "scale_labels = []\n",
    "\n",
    "for idx, (name, df, unit) in enumerate(datasets_info):\n",
    "    numerical_cols = [col for col in df.columns if col != 'pose_id']\n",
    "    \n",
    "    # Calculate range (max - min) for each feature\n",
    "    ranges = []\n",
    "    for col in numerical_cols:\n",
    "        col_range = df[col].max() - df[col].min()\n",
    "        ranges.append(col_range)\n",
    "    \n",
    "    scale_data.append(ranges)\n",
    "    scale_labels.append(name)\n",
    "    \n",
    "    # Histogram of feature ranges\n",
    "    axes[idx].hist(ranges, bins=15, alpha=0.7, color=sns.color_palette(\"husl\")[idx], edgecolor='black')\n",
    "    axes[idx].set_title(f'{name} Feature Ranges', fontweight='bold')\n",
    "    axes[idx].set_xlabel(f'Range ({unit})')\n",
    "    axes[idx].set_ylabel('Number of Features')\n",
    "    \n",
    "    # Add statistics\n",
    "    axes[idx].axvline(np.mean(ranges), color='red', linestyle='--', linewidth=2, \n",
    "                     label=f'Mean: {np.mean(ranges):.1f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.3 Comparative Statistics Table\n",
    "print(\"\\n4.3 Comparative Statistics Summary\")\n",
    "stats_summary = []\n",
    "\n",
    "for name, df, unit in datasets_info:\n",
    "    numerical_cols = [col for col in df.columns if col != 'pose_id']\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    all_values = []\n",
    "    for col in numerical_cols:\n",
    "        all_values.extend(df[col].values)\n",
    "    \n",
    "    stats = {\n",
    "        'Dataset': name,\n",
    "        'Features': len(numerical_cols),\n",
    "        'Total Values': len(all_values),\n",
    "        'Mean': np.mean(all_values),\n",
    "        'Std': np.std(all_values),\n",
    "        'Min': np.min(all_values),\n",
    "        'Max': np.max(all_values),\n",
    "        'Range': np.max(all_values) - np.min(all_values),\n",
    "        'Unit': unit\n",
    "    }\n",
    "    stats_summary.append(stats)\n",
    "\n",
    "import pandas as pd\n",
    "stats_df = pd.DataFrame(stats_summary)\n",
    "print(stats_df.round(2).to_string(index=False))\n",
    "\n",
    "# 4.4 Data Quality Assessment\n",
    "print(f\"\\n4.4 Data Quality Assessment\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "quality_metrics = []\n",
    "for name, df, unit in datasets_info:\n",
    "    numerical_cols = [col for col in df.columns if col != 'pose_id']\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df[numerical_cols].isnull().sum().sum()\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_count = 0\n",
    "    for col in numerical_cols:\n",
    "        inf_count += np.isinf(df[col]).sum()\n",
    "    \n",
    "    # Check for zero variance features\n",
    "    zero_var_count = 0\n",
    "    for col in numerical_cols:\n",
    "        if df[col].var() == 0:\n",
    "            zero_var_count += 1\n",
    "    \n",
    "    quality_metrics.append({\n",
    "        'Dataset': name,\n",
    "        'Missing Values': missing_count,\n",
    "        'Infinite Values': inf_count,\n",
    "        'Zero Variance Features': zero_var_count,\n",
    "        'Quality Score': f\"{((len(numerical_cols) - zero_var_count) / len(numerical_cols) * 100):.1f}%\"\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_metrics)\n",
    "print(quality_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nDataset Comparison Summary:\")\n",
    "print(f\"• Landmark coordinates have the highest variability (largest ranges)\")\n",
    "print(f\"• Angle features are bounded (0-180°) with moderate variability\") \n",
    "print(f\"• Distance features show right-skewed distributions\")\n",
    "print(f\"• All datasets are complete with no missing values\")\n",
    "print(f\"• Feature scaling will be important for ML algorithms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 5. POSE-SPECIFIC ANALYSIS ==============\n",
    "print(\"\\n5. POSE-SPECIFIC FEATURE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get unique poses\n",
    "pose_classes = merged_df['pose'].unique()\n",
    "print(f\"Analyzing {len(pose_classes)} pose classes: {pose_classes}\")\n",
    "\n",
    "# 5.1 Pose-specific feature distributions\n",
    "print(\"\\n5.1 Feature Distributions by Pose Class\")\n",
    "\n",
    "# Select key features for analysis\n",
    "key_features = {\n",
    "    'Angle Features': angle_features[:2],\n",
    "    'Distance Features': distance_3d_features[:2]\n",
    "}\n",
    "\n",
    "for feature_type, features in key_features.items():\n",
    "    fig, axes = plt.subplots(1, len(features), figsize=(15, 6))\n",
    "    if len(features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f'{feature_type} Distribution by Pose Class', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        # Box plot by pose class\n",
    "        pose_data = [merged_df[merged_df['pose'] == pose][feature].values for pose in pose_classes]\n",
    "        \n",
    "        axes[idx].boxplot(pose_data, labels=pose_classes, patch_artist=True,\n",
    "                         boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "        axes[idx].set_title(f'{feature}', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Pose Classes')\n",
    "        axes[idx].set_ylabel('Feature Value')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add median values\n",
    "        medians = [np.median(data) for data in pose_data]\n",
    "        for i, median in enumerate(medians):\n",
    "            axes[idx].text(i+1, median, f'{median:.1f}', ha='center', va='bottom', \n",
    "                          bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5.2 Statistical significance between poses\n",
    "print(\"\\n5.2 Statistical Differences Between Poses\")\n",
    "from scipy import stats\n",
    "\n",
    "# Perform ANOVA test for each feature type\n",
    "anova_results = []\n",
    "\n",
    "for feature_type, features in key_features.items():\n",
    "    print(f\"\\n{feature_type} ANOVA Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for feature in features:\n",
    "        # Group data by pose\n",
    "        pose_groups = [merged_df[merged_df['pose'] == pose][feature].values for pose in pose_classes]\n",
    "        \n",
    "        # Perform one-way ANOVA\n",
    "        f_stat, p_value = stats.f_oneway(*pose_groups)\n",
    "        \n",
    "        anova_results.append({\n",
    "            'Feature Type': feature_type,\n",
    "            'Feature': feature,\n",
    "            'F-statistic': f_stat,\n",
    "            'p-value': p_value,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "        \n",
    "        print(f\"{feature:<35}: F={f_stat:.3f}, p={p_value:.6f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''}\")\n",
    "\n",
    "# 5.3 Pose discrimination visualization\n",
    "print(\"\\n5.3 Pose Class Discrimination Analysis\")\n",
    "\n",
    "# Create a 2D scatter plot using the two most discriminative features\n",
    "discriminative_features = [result['Feature'] for result in anova_results if result['p-value'] < 0.001][:2]\n",
    "\n",
    "if len(discriminative_features) >= 2:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = sns.color_palette(\"husl\", len(pose_classes))\n",
    "    for i, pose in enumerate(pose_classes):\n",
    "        pose_data = merged_df[merged_df['pose'] == pose]\n",
    "        plt.scatter(pose_data[discriminative_features[0]], pose_data[discriminative_features[1]], \n",
    "                   c=[colors[i]], label=pose, alpha=0.7, s=50)\n",
    "    \n",
    "    plt.xlabel(discriminative_features[0], fontweight='bold')\n",
    "    plt.ylabel(discriminative_features[1], fontweight='bold')\n",
    "    plt.title('Pose Class Discrimination using Most Significant Features', fontweight='bold', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5.4 Feature importance for pose classification\n",
    "print(\"\\n5.4 Feature Importance Summary\")\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "anova_df = anova_df.sort_values('p-value').reset_index(drop=True)\n",
    "\n",
    "print(\"Top 10 Most Discriminative Features:\")\n",
    "print(anova_df.head(10)[['Feature', 'F-statistic', 'p-value', 'Significant']].to_string(index=False))\n",
    "\n",
    "# 5.5 Pose class separability heatmap\n",
    "print(\"\\n5.5 Pose Class Separability Matrix\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate pairwise separability using most discriminative feature\n",
    "if discriminative_features:\n",
    "    separability_matrix = np.zeros((len(pose_classes), len(pose_classes)))\n",
    "    \n",
    "    for i, pose1 in enumerate(pose_classes):\n",
    "        for j, pose2 in enumerate(pose_classes):\n",
    "            if i != j:\n",
    "                group1 = merged_df[merged_df['pose'] == pose1][discriminative_features[0]].values\n",
    "                group2 = merged_df[merged_df['pose'] == pose2][discriminative_features[0]].values\n",
    "                \n",
    "                # Calculate Cohen's d (effect size)\n",
    "                pooled_std = np.sqrt(((len(group1) - 1) * np.var(group1) + (len(group2) - 1) * np.var(group2)) / (len(group1) + len(group2) - 2))\n",
    "                cohens_d = abs(np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "                separability_matrix[i, j] = cohens_d\n",
    "            else:\n",
    "                separability_matrix[i, j] = 0\n",
    "    \n",
    "    sns.heatmap(separability_matrix, annot=True, cmap='YlOrRd', \n",
    "                xticklabels=pose_classes, yticklabels=pose_classes,\n",
    "                cbar_kws={'label': \"Cohen's d (Effect Size)\"})\n",
    "    plt.title(\"Pose Class Separability Matrix\\n(Cohen's d Effect Size)\", fontweight='bold', fontsize=14)\n",
    "    plt.xlabel('Pose Class')\n",
    "    plt.ylabel('Pose Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nPose-Specific Analysis Summary:\")\n",
    "print(f\"• {len([r for r in anova_results if r['p-value'] < 0.05])} out of {len(anova_results)} features show significant differences between poses\")\n",
    "print(f\"• Most discriminative features can be used for effective pose classification\")\n",
    "print(f\"• Some pose pairs may be more difficult to distinguish than others\")\n",
    "print(f\"• Feature selection based on statistical significance will improve model performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a730a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== VISUALIZATION SUMMARY & RECOMMENDATIONS ==============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"           STATISTICAL VISUALIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 ANALYSIS COMPLETED - KEY INSIGHTS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(f\"\\n1. DATASET COMPOSITION:\")\n",
    "print(f\"   • Total samples: {len(merged_df):,}\")\n",
    "print(f\"   • Feature types: Landmarks ({len(landmark_features)}), Angles ({len(angle_features)}), Distances ({len(distance_3d_features) + len(distance_xyz_features)})\")\n",
    "print(f\"   • Pose classes: {len(pose_classes)} different poses\")\n",
    "print(f\"   • Data quality: 100% complete, no missing values\")\n",
    "\n",
    "print(f\"\\n2. CLASS DISTRIBUTION:\")\n",
    "class_counts = merged_df['pose'].value_counts()\n",
    "print(f\"   • Most common pose: {class_counts.index[0]} ({class_counts.iloc[0]} samples)\")\n",
    "print(f\"   • Least common pose: {class_counts.index[-1]} ({class_counts.iloc[-1]} samples)\")\n",
    "balance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"   • Class balance ratio: {balance_ratio:.2f}:1 ({'Balanced' if balance_ratio <= 1.5 else 'Imbalanced'})\")\n",
    "\n",
    "print(f\"\\n3. FEATURE CHARACTERISTICS:\")\n",
    "print(f\"   • Landmark coordinates: High variability, continuous distributions\")\n",
    "print(f\"   • Joint angles: Bounded (0-180°), moderate variability\")\n",
    "print(f\"   • Distance features: Right-skewed, positive values only\")\n",
    "print(f\"   • Scale differences: Significant - normalization required\")\n",
    "\n",
    "if 'anova_results' in locals():\n",
    "    significant_features = len([r for r in anova_results if r['p-value'] < 0.05])\n",
    "    print(f\"\\n4. POSE DISCRIMINATION:\")\n",
    "    print(f\"   • Statistically significant features: {significant_features}/{len(anova_results)}\")\n",
    "    print(f\"   • Feature selection potential: High\")\n",
    "    print(f\"   • Classification feasibility: Excellent\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS FOR ML PIPELINE:\")\n",
    "print(f\"   ✓ Apply feature scaling (StandardScaler or MinMaxScaler)\")\n",
    "print(f\"   ✓ Consider feature selection based on ANOVA results\")\n",
    "print(f\"   ✓ Use correlation analysis to remove redundant features\")\n",
    "print(f\"   ✓ Consider PCA for dimensionality reduction if needed\")\n",
    "if balance_ratio > 1.5:\n",
    "    print(f\"   ⚠ Address class imbalance with SMOTE or class weights\")\n",
    "print(f\"   ✓ Dataset is ready for supervised learning algorithms\")\n",
    "\n",
    "print(f\"\\n6. SUGGESTED NEXT STEPS:\")\n",
    "print(f\"   1. Feature engineering based on domain knowledge\")\n",
    "print(f\"   2. Train-test split with stratification\")\n",
    "print(f\"   3. Implement feature scaling pipeline\") \n",
    "print(f\"   4. Try multiple classification algorithms\")\n",
    "print(f\"   5. Use cross-validation for robust evaluation\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"   🎯 DATASET IS WELL-PREPARED FOR POSTURE CORRECTION ML\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28382ce7",
   "metadata": {},
   "source": [
    "# ============== POSTER PRESENTATION GUIDE ==============\n",
    "print(\"=\"*70)\n",
    "print(\"           📊 POSTER PRESENTATION GUIDE\")\n",
    "print(\"       POSTURE CORRECTION DATASET ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "🎯 OVERVIEW:\n",
    "This presentation showcases a comprehensive statistical analysis of a posture \n",
    "correction dataset containing 1,372 samples with 170+ features across multiple \n",
    "data modalities for pose classification and correction tasks.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📈 VISUALIZATION 1: POSE CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 GRAPH NAMES:\n",
    "• \"Pose Class Distribution (Counts)\" - Bar Chart\n",
    "• \"Pose Class Distribution (Proportions)\" - Pie Chart  \n",
    "• \"Class Balance Analysis\" - Horizontal Bar Chart\n",
    "• \"Dataset Statistics Summary\" - Text Summary\n",
    "\n",
    "🗣️ WHAT TO SAY:\n",
    "\"Our first analysis examines the distribution of pose classes in our dataset. \n",
    "The bar chart shows the absolute count of samples for each pose type, while \n",
    "the pie chart illustrates the proportional distribution. We can see that our \n",
    "dataset contains [X] different pose classes with [describe balance]. This \n",
    "class distribution analysis is crucial for understanding potential bias in \n",
    "our training data and determining if sampling techniques like SMOTE will be \n",
    "needed to address any imbalances.\"\n",
    "\n",
    "🎯 KEY POINTS TO HIGHLIGHT:\n",
    "• Total number of pose classes\n",
    "• Most and least represented poses\n",
    "• Class balance ratio and implications for ML\n",
    "• Dataset size adequacy for deep learning\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📊 VISUALIZATION 2: FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 GRAPH NAMES:\n",
    "• \"Landmark Coordinates Distribution Analysis\" - Multi-panel Histograms\n",
    "• \"Joint Angles Distribution Analysis\" - Violin Plots with Box Plots\n",
    "• \"3D Distance Features Distribution\" - Histograms with KDE Overlay\n",
    "\n",
    "🗣️ WHAT TO SAY:\n",
    "\"These distributions reveal the characteristics of our three main feature types. \n",
    "The landmark coordinate histograms show the spatial distribution of body keypoints, \n",
    "with red dashed lines indicating means and orange lines showing standard deviations. \n",
    "The joint angle violin plots demonstrate the natural range of human joint flexibility, \n",
    "typically bounded between 0 and 180 degrees. The distance feature distributions with \n",
    "KDE overlays show right-skewed patterns, which is expected for distance measurements \n",
    "as they cannot be negative.\"\n",
    "\n",
    "🎯 KEY POINTS TO HIGHLIGHT:\n",
    "• Different statistical properties of each feature type\n",
    "• Normal vs. skewed distributions and their implications\n",
    "• Feature scaling requirements\n",
    "• Biological plausibility of the ranges\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🔥 VISUALIZATION 3: CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 GRAPH NAMES:\n",
    "• \"Joint Angles Correlation Matrix\" - Heatmap\n",
    "• \"Distribution of Correlation Coefficients\" - Histogram\n",
    "• \"Distance Features Correlation Matrix\" - Triangular Heatmap\n",
    "• \"Cross-Feature Type Correlation Analysis\" - Comprehensive Heatmap\n",
    "\n",
    "🗣️ WHAT TO SAY:\n",
    "\"The correlation analysis reveals important relationships between features. The joint \n",
    "angles correlation matrix uses a red-blue color scheme where darker blues indicate \n",
    "strong positive correlations and darker reds show negative correlations. The correlation \n",
    "coefficient distribution histogram helps us understand the overall correlation structure. \n",
    "High correlations (above 0.7) suggest potential feature redundancy, which could benefit \n",
    "from dimensionality reduction techniques like PCA.\"\n",
    "\n",
    "🎯 KEY POINTS TO HIGHLIGHT:\n",
    "• Identification of highly correlated feature pairs\n",
    "• Implications for feature selection\n",
    "• Redundancy reduction opportunities\n",
    "• Multi-modal feature relationships\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📏 VISUALIZATION 4: DATASET COMPARISON ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 GRAPH NAMES:\n",
    "• \"Feature Value Ranges Comparison Across Datasets\" - Multi-panel Box Plots\n",
    "• \"Feature Scale Distribution Across Dataset Types\" - Histogram Grid\n",
    "• \"Comparative Statistics Summary\" - Data Table\n",
    "• \"Data Quality Assessment\" - Quality Metrics Table\n",
    "\n",
    "🗣️ WHAT TO SAY:\n",
    "\"This comparative analysis highlights the different scales and ranges across our \n",
    "feature types. The box plots show that landmark coordinates have the widest value \n",
    "ranges, while joint angles are naturally bounded. The scale distribution histograms \n",
    "reveal significant differences in feature magnitudes, emphasizing the critical need \n",
    "for feature normalization in our ML pipeline. Our quality assessment shows 100% \n",
    "data completeness with no missing values.\"\n",
    "\n",
    "🎯 KEY POINTS TO HIGHLIGHT:\n",
    "• Scale differences requiring normalization\n",
    "• Data quality and completeness\n",
    "• Feature range characteristics\n",
    "• Preprocessing requirements\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎯 VISUALIZATION 5: POSE-SPECIFIC ANALYSIS (MOST IMPORTANT)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 GRAPH NAMES:\n",
    "• \"[Feature Type] Distribution by Pose Class\" - Grouped Box Plots\n",
    "• \"Statistical Differences Between Poses (ANOVA Results)\" - Statistical Table\n",
    "• \"Pose Class Discrimination using Most Significant Features\" - Scatter Plot\n",
    "• \"Pose Class Separability Matrix (Cohen's d Effect Size)\" - Heatmap\n",
    "\n",
    "🗣️ WHAT TO SAY:\n",
    "\"This is our most critical analysis for pose classification. The grouped box plots \n",
    "show how feature values differ across pose classes, with median values clearly labeled. \n",
    "Our ANOVA statistical testing identifies which features show significant differences \n",
    "between poses (p < 0.05), with stars indicating significance levels. The discrimination \n",
    "scatter plot uses the two most statistically significant features to visualize pose \n",
    "separability in 2D space. Finally, the separability matrix quantifies how well each \n",
    "pose pair can be distinguished using Cohen's d effect size.\"\n",
    "\n",
    "🎯 KEY POINTS TO HIGHLIGHT:\n",
    "• Statistical significance of features for classification\n",
    "• Best discriminative features identified\n",
    "• Pose pair difficulty assessment\n",
    "• Feature selection guidance for ML models\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📋 PRESENTATION STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "🎤 OPENING STATEMENT:\n",
    "\"Today I'll present a comprehensive statistical analysis of our posture correction \n",
    "dataset, examining 1,372 pose samples across multiple feature modalities to assess \n",
    "the feasibility and optimize the approach for machine learning-based pose \n",
    "classification and correction.\"\n",
    "\n",
    "📝 FOR EACH VISUALIZATION, FOLLOW THIS STRUCTURE:\n",
    "1. \"What am I showing?\" - Name the graph type and what it represents\n",
    "2. \"What does this tell us?\" - Interpret the key findings\n",
    "3. \"Why does this matter?\" - Connect to your research goals\n",
    "4. \"What's next?\" - How this informs your methodology\n",
    "\n",
    "🏁 CONCLUSION STATEMENT:\n",
    "\"Our comprehensive analysis demonstrates that this dataset is exceptionally well-suited \n",
    "for machine learning applications in posture correction. We have identified key \n",
    "discriminative features, confirmed data quality, and established preprocessing \n",
    "requirements that will guide our model development for accurate pose classification \n",
    "and real-time posture correction systems.\"\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎨 VISUAL PRESENTATION TIPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "🌈 COLOR SCHEMES USED:\n",
    "• Husl palette: Maximizes color distinction between categories\n",
    "• RdBu_r: Red-blue for correlation (intuitive positive/negative)\n",
    "• Viridis: Perceptually uniform for continuous data\n",
    "• YlOrRd: Yellow-orange-red for intensity/magnitude\n",
    "\n",
    "📖 GRAPH READING GUIDE:\n",
    "• Box plots: Center line = median, box = IQR, whiskers = 1.5×IQR\n",
    "• Violin plots: Width shows distribution density at each value\n",
    "• Heatmaps: Darker colors = stronger relationships\n",
    "• Scatter plots: Clustering indicates class separability\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📝 Q&A PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "❓ COMMON QUESTIONS & ANSWERS:\n",
    "\n",
    "Q: \"How do you handle the scale differences between features?\"\n",
    "A: \"Our analysis revealed significant scale differences, so we'll implement \n",
    "   StandardScaler normalization as part of our preprocessing pipeline.\"\n",
    "\n",
    "Q: \"Are there enough samples for deep learning?\"\n",
    "A: \"With 1,372 samples and clear feature discriminability, this is sufficient \n",
    "   for traditional ML. For deep learning, we could use data augmentation techniques.\"\n",
    "\n",
    "Q: \"Which features are most important for pose classification?\"\n",
    "A: \"Our ANOVA analysis identified [X] statistically significant features with \n",
    "   p-values < 0.001, which will form the core of our feature selection strategy.\"\n",
    "\n",
    "Q: \"What's your accuracy expectation?\"\n",
    "A: \"Given the clear feature separability shown in our analysis, we expect \n",
    "   classification accuracy above 85-90% with proper preprocessing.\"\n",
    "\n",
    "Q: \"How will this be used in real applications?\"\n",
    "A: \"The identified discriminative features will enable real-time pose classification \n",
    "   for automated posture correction feedback systems.\"\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✅ PRESENTATION GUIDE COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa327f",
   "metadata": {},
   "source": [
    "# 📊 POSTER PRESENTATION GUIDE: POSTURE CORRECTION DATASET ANALYSIS\n",
    "\n",
    "## Overview\n",
    "This presentation showcases a comprehensive statistical analysis of a posture correction dataset containing **1,372 samples** with **170+ features** across multiple data modalities for pose classification and correction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 VISUALIZATION 1: POSE CLASS DISTRIBUTION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Pose Class Distribution (Counts)\"** - Bar Chart\n",
    "- **\"Pose Class Distribution (Proportions)\"** - Pie Chart  \n",
    "- **\"Class Balance Analysis\"** - Horizontal Bar Chart\n",
    "- **\"Dataset Statistics Summary\"** - Text Summary\n",
    "\n",
    "### What to Say:\n",
    "*\"Our first analysis examines the distribution of pose classes in our dataset. The bar chart shows the absolute count of samples for each pose type, while the pie chart illustrates the proportional distribution. We can see that our dataset contains [X] different pose classes with [describe balance - balanced/imbalanced]. This class distribution analysis is crucial for understanding potential bias in our training data and determining if sampling techniques like SMOTE will be needed to address any imbalances.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Total number of pose classes\n",
    "- Most and least represented poses\n",
    "- Class balance ratio and implications for ML\n",
    "- Dataset size adequacy for deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 VISUALIZATION 2: FEATURE DISTRIBUTION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Landmark Coordinates Distribution Analysis\"** - Multi-panel Histograms\n",
    "- **\"Joint Angles Distribution Analysis\"** - Violin Plots with Box Plots\n",
    "- **\"3D Distance Features Distribution\"** - Histograms with KDE Overlay\n",
    "\n",
    "### What to Say:\n",
    "*\"These distributions reveal the characteristics of our three main feature types. The landmark coordinate histograms show the spatial distribution of body keypoints, with red dashed lines indicating means and orange lines showing standard deviations. The joint angle violin plots demonstrate the natural range of human joint flexibility, typically bounded between 0 and 180 degrees. The distance feature distributions with KDE overlays show right-skewed patterns, which is expected for distance measurements as they cannot be negative.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Different statistical properties of each feature type\n",
    "- Normal vs. skewed distributions and their implications\n",
    "- Feature scaling requirements\n",
    "- Biological plausibility of the ranges\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 VISUALIZATION 3: CORRELATION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Joint Angles Correlation Matrix\"** - Heatmap\n",
    "- **\"Distribution of Correlation Coefficients\"** - Histogram\n",
    "- **\"Distance Features Correlation Matrix\"** - Triangular Heatmap\n",
    "- **\"Cross-Feature Type Correlation Analysis\"** - Comprehensive Heatmap\n",
    "\n",
    "### What to Say:\n",
    "*\"The correlation analysis reveals important relationships between features. The joint angles correlation matrix uses a red-blue color scheme where darker blues indicate strong positive correlations and darker reds show negative correlations. The correlation coefficient distribution histogram helps us understand the overall correlation structure. High correlations (above 0.7) suggest potential feature redundancy, which could benefit from dimensionality reduction techniques like PCA. The cross-feature type analysis shows how different modalities (angles, landmarks, distances) relate to each other.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Identification of highly correlated feature pairs\n",
    "- Implications for feature selection\n",
    "- Redundancy reduction opportunities\n",
    "- Multi-modal feature relationships\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 VISUALIZATION 4: DATASET COMPARISON ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Feature Value Ranges Comparison Across Datasets\"** - Multi-panel Box Plots\n",
    "- **\"Feature Scale Distribution Across Dataset Types\"** - Histogram Grid\n",
    "- **\"Comparative Statistics Summary\"** - Data Table\n",
    "- **\"Data Quality Assessment\"** - Quality Metrics Table\n",
    "\n",
    "### What to Say:\n",
    "*\"This comparative analysis highlights the different scales and ranges across our feature types. The box plots show that landmark coordinates have the widest value ranges, while joint angles are naturally bounded. The scale distribution histograms reveal significant differences in feature magnitudes, emphasizing the critical need for feature normalization in our ML pipeline. Our quality assessment shows 100% data completeness with no missing values, indicating a high-quality dataset ready for machine learning applications.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Scale differences requiring normalization\n",
    "- Data quality and completeness\n",
    "- Feature range characteristics\n",
    "- Preprocessing requirements\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 VISUALIZATION 5: POSE-SPECIFIC ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"[Feature Type] Distribution by Pose Class\"** - Grouped Box Plots\n",
    "- **\"Statistical Differences Between Poses (ANOVA Results)\"** - Statistical Table\n",
    "- **\"Pose Class Discrimination using Most Significant Features\"** - Scatter Plot\n",
    "- **\"Pose Class Separability Matrix (Cohen's d Effect Size)\"** - Heatmap\n",
    "\n",
    "### What to Say:\n",
    "*\"This is our most critical analysis for pose classification. The grouped box plots show how feature values differ across pose classes, with median values clearly labeled. Our ANOVA statistical testing identifies which features show significant differences between poses (p < 0.05), with stars indicating significance levels. The discrimination scatter plot uses the two most statistically significant features to visualize pose separability in 2D space. Finally, the separability matrix quantifies how well each pose pair can be distinguished using Cohen's d effect size, where larger values indicate better separability.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Statistical significance of features for classification\n",
    "- Best discriminative features identified\n",
    "- Pose pair difficulty assessment\n",
    "- Feature selection guidance for ML models\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 PRESENTATION TALKING POINTS\n",
    "\n",
    "### Opening Statement:\n",
    "*\"Today I'll present a comprehensive statistical analysis of our posture correction dataset, examining 1,372 pose samples across multiple feature modalities to assess the feasibility and optimize the approach for machine learning-based pose classification and correction.\"*\n",
    "\n",
    "### For Each Visualization, Follow This Structure:\n",
    "\n",
    "1. **\"What am I showing?\"** - Name the graph type and what it represents\n",
    "2. **\"What does this tell us?\"** - Interpret the key findings\n",
    "3. **\"Why does this matter?\"** - Connect to your research goals\n",
    "4. **\"What's next?\"** - How this informs your methodology\n",
    "\n",
    "### Conclusion Statement:\n",
    "*\"Our comprehensive analysis demonstrates that this dataset is exceptionally well-suited for machine learning applications in posture correction. We have identified key discriminative features, confirmed data quality, and established preprocessing requirements that will guide our model development for accurate pose classification and real-time posture correction systems.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 VISUAL PRESENTATION TIPS\n",
    "\n",
    "### Color Schemes Used:\n",
    "- **Husl palette**: Maximizes color distinction between categories\n",
    "- **RdBu_r**: Red-blue for correlation (intuitive positive/negative)\n",
    "- **Viridis**: Perceptually uniform for continuous data\n",
    "- **YlOrRd**: Yellow-orange-red for intensity/magnitude\n",
    "\n",
    "### Graph Reading Guide:\n",
    "- **Box plots**: Center line = median, box = IQR, whiskers = 1.5×IQR\n",
    "- **Violin plots**: Width shows distribution density at each value\n",
    "- **Heatmaps**: Darker colors = stronger relationships\n",
    "- **Scatter plots**: Clustering indicates class separability\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 SUGGESTED Q&A PREPARATION\n",
    "\n",
    "**Q: \"How do you handle the scale differences between features?\"**\n",
    "*A: \"Our analysis revealed significant scale differences, so we'll implement StandardScaler normalization as part of our preprocessing pipeline.\"*\n",
    "\n",
    "**Q: \"Are there enough samples for deep learning?\"**\n",
    "*A: \"With 1,372 samples and clear feature discriminability, this is sufficient for traditional ML. For deep learning, we could use data augmentation techniques.\"*\n",
    "\n",
    "**Q: \"Which features are most important for pose classification?\"**\n",
    "*A: \"Our ANOVA analysis identified [X] statistically significant features with p-values < 0.001, which will form the core of our feature selection strategy.\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbf5e3",
   "metadata": {},
   "source": [
    "# 📊 POSTER PRESENTATION GUIDE: POSTURE CORRECTION DATASET ANALYSIS\n",
    "\n",
    "## Overview\n",
    "This presentation showcases a comprehensive statistical analysis of a posture correction dataset containing **1,372 samples** with **170+ features** across multiple data modalities for pose classification and correction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 VISUALIZATION 1: POSE CLASS DISTRIBUTION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Pose Class Distribution (Counts)\"** - Bar Chart\n",
    "- **\"Pose Class Distribution (Proportions)\"** - Pie Chart  \n",
    "- **\"Class Balance Analysis\"** - Horizontal Bar Chart\n",
    "- **\"Dataset Statistics Summary\"** - Text Summary\n",
    "\n",
    "### What to Say:\n",
    "*\"Our first analysis examines the distribution of pose classes in our dataset. The bar chart shows the absolute count of samples for each pose type, while the pie chart illustrates the proportional distribution. We can see that our dataset contains [X] different pose classes with [describe balance - balanced/imbalanced]. This class distribution analysis is crucial for understanding potential bias in our training data and determining if sampling techniques like SMOTE will be needed to address any imbalances.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Total number of pose classes\n",
    "- Most and least represented poses\n",
    "- Class balance ratio and implications for ML\n",
    "- Dataset size adequacy for deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 VISUALIZATION 2: FEATURE DISTRIBUTION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Landmark Coordinates Distribution Analysis\"** - Multi-panel Histograms\n",
    "- **\"Joint Angles Distribution Analysis\"** - Violin Plots with Box Plots\n",
    "- **\"3D Distance Features Distribution\"** - Histograms with KDE Overlay\n",
    "\n",
    "### What to Say:\n",
    "*\"These distributions reveal the characteristics of our three main feature types. The landmark coordinate histograms show the spatial distribution of body keypoints, with red dashed lines indicating means and orange lines showing standard deviations. The joint angle violin plots demonstrate the natural range of human joint flexibility, typically bounded between 0 and 180 degrees. The distance feature distributions with KDE overlays show right-skewed patterns, which is expected for distance measurements as they cannot be negative.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Different statistical properties of each feature type\n",
    "- Normal vs. skewed distributions and their implications\n",
    "- Feature scaling requirements\n",
    "- Biological plausibility of the ranges\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 VISUALIZATION 3: CORRELATION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Joint Angles Correlation Matrix\"** - Heatmap\n",
    "- **\"Distribution of Correlation Coefficients\"** - Histogram\n",
    "- **\"Distance Features Correlation Matrix\"** - Triangular Heatmap\n",
    "- **\"Cross-Feature Type Correlation Analysis\"** - Comprehensive Heatmap\n",
    "\n",
    "### What to Say:\n",
    "*\"The correlation analysis reveals important relationships between features. The joint angles correlation matrix uses a red-blue color scheme where darker blues indicate strong positive correlations and darker reds show negative correlations. The correlation coefficient distribution histogram helps us understand the overall correlation structure. High correlations (above 0.7) suggest potential feature redundancy, which could benefit from dimensionality reduction techniques like PCA. The cross-feature type analysis shows how different modalities (angles, landmarks, distances) relate to each other.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Identification of highly correlated feature pairs\n",
    "- Implications for feature selection\n",
    "- Redundancy reduction opportunities\n",
    "- Multi-modal feature relationships\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 VISUALIZATION 4: DATASET COMPARISON ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Feature Value Ranges Comparison Across Datasets\"** - Multi-panel Box Plots\n",
    "- **\"Feature Scale Distribution Across Dataset Types\"** - Histogram Grid\n",
    "- **\"Comparative Statistics Summary\"** - Data Table\n",
    "- **\"Data Quality Assessment\"** - Quality Metrics Table\n",
    "\n",
    "### What to Say:\n",
    "*\"This comparative analysis highlights the different scales and ranges across our feature types. The box plots show that landmark coordinates have the widest value ranges, while joint angles are naturally bounded. The scale distribution histograms reveal significant differences in feature magnitudes, emphasizing the critical need for feature normalization in our ML pipeline. Our quality assessment shows 100% data completeness with no missing values, indicating a high-quality dataset ready for machine learning applications.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Scale differences requiring normalization\n",
    "- Data quality and completeness\n",
    "- Feature range characteristics\n",
    "- Preprocessing requirements\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 VISUALIZATION 5: POSE-SPECIFIC ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"[Feature Type] Distribution by Pose Class\"** - Grouped Box Plots\n",
    "- **\"Statistical Differences Between Poses (ANOVA Results)\"** - Statistical Table\n",
    "- **\"Pose Class Discrimination using Most Significant Features\"** - Scatter Plot\n",
    "- **\"Pose Class Separability Matrix (Cohen's d Effect Size)\"** - Heatmap\n",
    "\n",
    "### What to Say:\n",
    "*\"This is our most critical analysis for pose classification. The grouped box plots show how feature values differ across pose classes, with median values clearly labeled. Our ANOVA statistical testing identifies which features show significant differences between poses (p < 0.05), with stars indicating significance levels. The discrimination scatter plot uses the two most statistically significant features to visualize pose separability in 2D space. Finally, the separability matrix quantifies how well each pose pair can be distinguished using Cohen's d effect size, where larger values indicate better separability.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Statistical significance of features for classification\n",
    "- Best discriminative features identified\n",
    "- Pose pair difficulty assessment\n",
    "- Feature selection guidance for ML models\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 PRESENTATION TALKING POINTS\n",
    "\n",
    "### Opening Statement:\n",
    "*\"Today I'll present a comprehensive statistical analysis of our posture correction dataset, examining 1,372 pose samples across multiple feature modalities to assess the feasibility and optimize the approach for machine learning-based pose classification and correction.\"*\n",
    "\n",
    "### For Each Visualization, Follow This Structure:\n",
    "\n",
    "1. **\"What am I showing?\"** - Name the graph type and what it represents\n",
    "2. **\"What does this tell us?\"** - Interpret the key findings\n",
    "3. **\"Why does this matter?\"** - Connect to your research goals\n",
    "4. **\"What's next?\"** - How this informs your methodology\n",
    "\n",
    "### Conclusion Statement:\n",
    "*\"Our comprehensive analysis demonstrates that this dataset is exceptionally well-suited for machine learning applications in posture correction. We have identified key discriminative features, confirmed data quality, and established preprocessing requirements that will guide our model development for accurate pose classification and real-time posture correction systems.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 VISUAL PRESENTATION TIPS\n",
    "\n",
    "### Color Schemes Used:\n",
    "- **Husl palette**: Maximizes color distinction between categories\n",
    "- **RdBu_r**: Red-blue for correlation (intuitive positive/negative)\n",
    "- **Viridis**: Perceptually uniform for continuous data\n",
    "- **YlOrRd**: Yellow-orange-red for intensity/magnitude\n",
    "\n",
    "### Graph Reading Guide:\n",
    "- **Box plots**: Center line = median, box = IQR, whiskers = 1.5×IQR\n",
    "- **Violin plots**: Width shows distribution density at each value\n",
    "- **Heatmaps**: Darker colors = stronger relationships\n",
    "- **Scatter plots**: Clustering indicates class separability\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 SUGGESTED Q&A PREPARATION\n",
    "\n",
    "**Q: \"How do you handle the scale differences between features?\"**\n",
    "*A: \"Our analysis revealed significant scale differences, so we'll implement StandardScaler normalization as part of our preprocessing pipeline.\"*\n",
    "\n",
    "**Q: \"Are there enough samples for deep learning?\"**\n",
    "*A: \"With 1,372 samples and clear feature discriminability, this is sufficient for traditional ML. For deep learning, we could use data augmentation techniques.\"*\n",
    "\n",
    "**Q: \"Which features are most important for pose classification?\"**\n",
    "*A: \"Our ANOVA analysis identified [X] statistically significant features with p-values < 0.001, which will form the core of our feature selection strategy.\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd3547",
   "metadata": {},
   "source": [
    "# 📊 POSTER PRESENTATION GUIDE: POSTURE CORRECTION DATASET ANALYSIS\n",
    "\n",
    "## Overview\n",
    "This presentation showcases a comprehensive statistical analysis of a posture correction dataset containing **1,372 samples** with **170+ features** across multiple data modalities for pose classification and correction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 VISUALIZATION 1: POSE CLASS DISTRIBUTION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Pose Class Distribution (Counts)\"** - Bar Chart\n",
    "- **\"Pose Class Distribution (Proportions)\"** - Pie Chart  \n",
    "- **\"Class Balance Analysis\"** - Horizontal Bar Chart\n",
    "- **\"Dataset Statistics Summary\"** - Text Summary\n",
    "\n",
    "### What to Say:\n",
    "*\"Our first analysis examines the distribution of pose classes in our dataset. The bar chart shows the absolute count of samples for each pose type, while the pie chart illustrates the proportional distribution. We can see that our dataset contains [X] different pose classes with [describe balance - balanced/imbalanced]. This class distribution analysis is crucial for understanding potential bias in our training data and determining if sampling techniques like SMOTE will be needed to address any imbalances.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Total number of pose classes\n",
    "- Most and least represented poses\n",
    "- Class balance ratio and implications for ML\n",
    "- Dataset size adequacy for deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 VISUALIZATION 2: FEATURE DISTRIBUTION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Landmark Coordinates Distribution Analysis\"** - Multi-panel Histograms\n",
    "- **\"Joint Angles Distribution Analysis\"** - Violin Plots with Box Plots\n",
    "- **\"3D Distance Features Distribution\"** - Histograms with KDE Overlay\n",
    "\n",
    "### What to Say:\n",
    "*\"These distributions reveal the characteristics of our three main feature types. The landmark coordinate histograms show the spatial distribution of body keypoints, with red dashed lines indicating means and orange lines showing standard deviations. The joint angle violin plots demonstrate the natural range of human joint flexibility, typically bounded between 0 and 180 degrees. The distance feature distributions with KDE overlays show right-skewed patterns, which is expected for distance measurements as they cannot be negative.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Different statistical properties of each feature type\n",
    "- Normal vs. skewed distributions and their implications\n",
    "- Feature scaling requirements\n",
    "- Biological plausibility of the ranges\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 VISUALIZATION 3: CORRELATION ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Joint Angles Correlation Matrix\"** - Heatmap\n",
    "- **\"Distribution of Correlation Coefficients\"** - Histogram\n",
    "- **\"Distance Features Correlation Matrix\"** - Triangular Heatmap\n",
    "- **\"Cross-Feature Type Correlation Analysis\"** - Comprehensive Heatmap\n",
    "\n",
    "### What to Say:\n",
    "*\"The correlation analysis reveals important relationships between features. The joint angles correlation matrix uses a red-blue color scheme where darker blues indicate strong positive correlations and darker reds show negative correlations. The correlation coefficient distribution histogram helps us understand the overall correlation structure. High correlations (above 0.7) suggest potential feature redundancy, which could benefit from dimensionality reduction techniques like PCA. The cross-feature type analysis shows how different modalities (angles, landmarks, distances) relate to each other.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Identification of highly correlated feature pairs\n",
    "- Implications for feature selection\n",
    "- Redundancy reduction opportunities\n",
    "- Multi-modal feature relationships\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 VISUALIZATION 4: DATASET COMPARISON ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"Feature Value Ranges Comparison Across Datasets\"** - Multi-panel Box Plots\n",
    "- **\"Feature Scale Distribution Across Dataset Types\"** - Histogram Grid\n",
    "- **\"Comparative Statistics Summary\"** - Data Table\n",
    "- **\"Data Quality Assessment\"** - Quality Metrics Table\n",
    "\n",
    "### What to Say:\n",
    "*\"This comparative analysis highlights the different scales and ranges across our feature types. The box plots show that landmark coordinates have the widest value ranges, while joint angles are naturally bounded. The scale distribution histograms reveal significant differences in feature magnitudes, emphasizing the critical need for feature normalization in our ML pipeline. Our quality assessment shows 100% data completeness with no missing values, indicating a high-quality dataset ready for machine learning applications.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Scale differences requiring normalization\n",
    "- Data quality and completeness\n",
    "- Feature range characteristics\n",
    "- Preprocessing requirements\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 VISUALIZATION 5: POSE-SPECIFIC ANALYSIS\n",
    "\n",
    "### Graph Names:\n",
    "- **\"[Feature Type] Distribution by Pose Class\"** - Grouped Box Plots\n",
    "- **\"Statistical Differences Between Poses (ANOVA Results)\"** - Statistical Table\n",
    "- **\"Pose Class Discrimination using Most Significant Features\"** - Scatter Plot\n",
    "- **\"Pose Class Separability Matrix (Cohen's d Effect Size)\"** - Heatmap\n",
    "\n",
    "### What to Say:\n",
    "*\"This is our most critical analysis for pose classification. The grouped box plots show how feature values differ across pose classes, with median values clearly labeled. Our ANOVA statistical testing identifies which features show significant differences between poses (p < 0.05), with stars indicating significance levels. The discrimination scatter plot uses the two most statistically significant features to visualize pose separability in 2D space. Finally, the separability matrix quantifies how well each pose pair can be distinguished using Cohen's d effect size, where larger values indicate better separability.\"*\n",
    "\n",
    "### Key Points to Highlight:\n",
    "- Statistical significance of features for classification\n",
    "- Best discriminative features identified\n",
    "- Pose pair difficulty assessment\n",
    "- Feature selection guidance for ML models\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 PRESENTATION TALKING POINTS\n",
    "\n",
    "### Opening Statement:\n",
    "*\"Today I'll present a comprehensive statistical analysis of our posture correction dataset, examining 1,372 pose samples across multiple feature modalities to assess the feasibility and optimize the approach for machine learning-based pose classification and correction.\"*\n",
    "\n",
    "### For Each Visualization, Follow This Structure:\n",
    "\n",
    "1. **\"What am I showing?\"** - Name the graph type and what it represents\n",
    "2. **\"What does this tell us?\"** - Interpret the key findings\n",
    "3. **\"Why does this matter?\"** - Connect to your research goals\n",
    "4. **\"What's next?\"** - How this informs your methodology\n",
    "\n",
    "### Conclusion Statement:\n",
    "*\"Our comprehensive analysis demonstrates that this dataset is exceptionally well-suited for machine learning applications in posture correction. We have identified key discriminative features, confirmed data quality, and established preprocessing requirements that will guide our model development for accurate pose classification and real-time posture correction systems.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 VISUAL PRESENTATION TIPS\n",
    "\n",
    "### Color Schemes Used:\n",
    "- **Husl palette**: Maximizes color distinction between categories\n",
    "- **RdBu_r**: Red-blue for correlation (intuitive positive/negative)\n",
    "- **Viridis**: Perceptually uniform for continuous data\n",
    "- **YlOrRd**: Yellow-orange-red for intensity/magnitude\n",
    "\n",
    "### Graph Reading Guide:\n",
    "- **Box plots**: Center line = median, box = IQR, whiskers = 1.5×IQR\n",
    "- **Violin plots**: Width shows distribution density at each value\n",
    "- **Heatmaps**: Darker colors = stronger relationships\n",
    "- **Scatter plots**: Clustering indicates class separability\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 SUGGESTED Q&A PREPARATION\n",
    "\n",
    "**Q: \"How do you handle the scale differences between features?\"**\n",
    "*A: \"Our analysis revealed significant scale differences, so we'll implement StandardScaler normalization as part of our preprocessing pipeline.\"*\n",
    "\n",
    "**Q: \"Are there enough samples for deep learning?\"**\n",
    "*A: \"With 1,372 samples and clear feature discriminability, this is sufficient for traditional ML. For deep learning, we could use data augmentation techniques.\"*\n",
    "\n",
    "**Q: \"Which features are most important for pose classification?\"**\n",
    "*A: \"Our ANOVA analysis identified [X] statistically significant features with p-values < 0.001, which will form the core of our feature selection strategy.\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fb265",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 🎤 POSTER PRESENTATION SCRIPT\n",
    "\n",
    "## 🎯 INTRODUCTION (30 seconds)\n",
    "*\"Hello! I'm presenting a comprehensive statistical analysis of a posture correction dataset for machine learning applications. Our goal is to develop automated systems that can classify human poses and provide real-time posture correction feedback.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 SECTION 1: CLASS DISTRIBUTION (45 seconds)\n",
    "\n",
    "### Point to: Bar Chart and Pie Chart\n",
    "**Script:**\n",
    "*\"Let's start with our dataset composition. This bar chart shows we have [X] different pose classes with [Y] total samples. The pie chart reveals the proportional distribution - notice that we have [describe if balanced/imbalanced]. This horizontal bar chart provides the exact counts and percentages for each pose type.\"*\n",
    "\n",
    "**Key Numbers to Mention:**\n",
    "- Total samples: 1,372\n",
    "- Number of pose classes: [from your data]\n",
    "- Largest class: [X]% of data\n",
    "- Smallest class: [Y]% of data\n",
    "\n",
    "**Impact Statement:**\n",
    "*\"This balanced/imbalanced distribution will inform our sampling strategy for model training.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 SECTION 2: FEATURE CHARACTERISTICS (60 seconds)\n",
    "\n",
    "### Point to: Multi-panel Histograms, Violin Plots, KDE Plots\n",
    "**Script:**\n",
    "*\"Now let's examine our feature types. We have three main categories:\"*\n",
    "\n",
    "1. **Landmark Coordinates** *(point to histograms)*\n",
    "   *\"These histograms show the spatial distribution of body keypoints. The red dashed lines indicate means, and you can see some features follow normal distributions while others are skewed.\"*\n",
    "\n",
    "2. **Joint Angles** *(point to violin plots)*\n",
    "   *\"These violin plots reveal joint flexibility ranges. The white box plots inside show medians and quartiles, while the violin shape shows the full distribution density. Notice they're naturally bounded between 0 and 180 degrees.\"*\n",
    "\n",
    "3. **Distance Features** *(point to KDE plots)*\n",
    "   *\"These distance measurements show right-skewed distributions with the red KDE curves overlaying the histograms. This is expected since distances can't be negative.\"*\n",
    "\n",
    "**Impact Statement:**\n",
    "*\"These different distribution patterns indicate we'll need robust feature scaling in our preprocessing pipeline.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 SECTION 3: CORRELATION INSIGHTS (45 seconds)\n",
    "\n",
    "### Point to: Correlation Heatmaps\n",
    "**Script:**\n",
    "*\"Our correlation analysis reveals important feature relationships. In this joint angles correlation matrix, blue indicates positive correlations and red shows negative correlations. The histogram on the right shows most correlations are moderate, but we did identify [X] feature pairs with high correlation above 0.7.\"*\n",
    "\n",
    "**Point to: Cross-feature Correlation**\n",
    "*\"This comprehensive heatmap shows how different feature types relate to each other - angles, landmarks, and distances. The pattern suggests some redundancy that we can address through feature selection.\"*\n",
    "\n",
    "**Impact Statement:**\n",
    "*\"High correlations suggest opportunities for dimensionality reduction using PCA or feature selection techniques.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 SECTION 4: SCALE COMPARISON (30 seconds)\n",
    "\n",
    "### Point to: Box Plots and Statistics Table\n",
    "**Script:**\n",
    "*\"This comparison reveals dramatic scale differences between feature types. Landmark coordinates have the widest ranges, while angles are bounded. The statistics table quantifies these differences - notice the range column shows variations from [X] to [Y] across feature types.\"*\n",
    "\n",
    "**Point to: Quality Assessment Table**\n",
    "*\"Importantly, our quality assessment shows 100% data completeness with zero missing values.\"*\n",
    "\n",
    "**Impact Statement:**\n",
    "*\"These scale differences confirm that feature normalization is critical for our machine learning pipeline.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 SECTION 5: POSE DISCRIMINATION (75 seconds) - **MOST IMPORTANT**\n",
    "\n",
    "### Point to: Box Plots by Pose Class\n",
    "**Script:**\n",
    "*\"This is our most critical analysis. These grouped box plots show how feature values differ across pose classes. You can see clear separation between poses in key features, with median values labeled on each box.\"*\n",
    "\n",
    "### Point to: ANOVA Results Table\n",
    "*\"Our statistical testing using ANOVA identified [X] features with significant differences between poses at p < 0.05. The stars indicate significance levels - three stars mean p < 0.001, which indicates very strong discrimination power.\"*\n",
    "\n",
    "### Point to: Scatter Plot\n",
    "*\"This scatter plot uses the two most discriminative features to visualize pose separability in 2D space. Notice how well the different colored clusters separate - this suggests excellent classification potential.\"*\n",
    "\n",
    "### Point to: Separability Matrix\n",
    "*\"Finally, this heatmap quantifies pose pair separability using Cohen's d effect size. Darker colors indicate better separability. Values above 0.8 suggest large effect sizes, meaning those pose pairs are easily distinguishable.\"*\n",
    "\n",
    "**Impact Statement:**\n",
    "*\"These results confirm that our features provide excellent discrimination power for automated pose classification.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 CONCLUSION (30 seconds)\n",
    "*\"In summary, our comprehensive analysis demonstrates this dataset is exceptionally well-prepared for machine learning applications. We have identified the most discriminative features, confirmed data quality, and established clear preprocessing requirements. Our next steps include implementing feature scaling, applying the identified feature selection criteria, and developing pose classification models with the confidence that our statistical foundation is solid.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ PRESENTATION DELIVERY TIPS\n",
    "\n",
    "### Timing Breakdown (Total: 5-6 minutes)\n",
    "- Introduction: 30 seconds\n",
    "- Section 1: 45 seconds  \n",
    "- Section 2: 60 seconds\n",
    "- Section 3: 45 seconds\n",
    "- Section 4: 30 seconds\n",
    "- Section 5: 75 seconds (most detailed)\n",
    "- Conclusion: 30 seconds\n",
    "- Q&A Buffer: 60 seconds\n",
    "\n",
    "### Physical Presentation Tips:\n",
    "1. **Use a pointer** - Point to specific parts of graphs while explaining\n",
    "2. **Face the audience** - Not the poster\n",
    "3. **Practice transitions** - \"Moving to our next analysis...\"\n",
    "4. **Emphasize key numbers** - Pause when stating important statistics\n",
    "5. **Use gestures** - Help explain concepts like \"separation\" or \"correlation\"\n",
    "\n",
    "### Voice Modulation:\n",
    "- **Excited tone** for good results (high separability, clean data)\n",
    "- **Analytical tone** for technical explanations\n",
    "- **Confident conclusion** about ML readiness\n",
    "\n",
    "### Common Questions to Prepare For:\n",
    "1. \"What's your sample size?\" → 1,372 samples\n",
    "2. \"How many features?\" → 170+ across multiple modalities  \n",
    "3. \"Any missing data?\" → Zero missing values, 100% complete\n",
    "4. \"Best features for classification?\" → [Your ANOVA results]\n",
    "5. \"What algorithms will you use?\" → [Your planned approach]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 2. CLUSTERING ALGORITHMS ==============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           🎲 CLUSTERING ALGORITHMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "n_classes = len(label_encoder.classes_)\n",
    "print(f\"Known number of pose classes: {n_classes}\")\n",
    "\n",
    "# K-Means Clustering\n",
    "print(f\"\\n📍 K-MEANS CLUSTERING\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Find optimal k using elbow method\n",
    "k_range = range(2, min(10, len(label_encoder.classes_) + 3))\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_train_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Silhouette score\n",
    "    labels = kmeans.predict(X_train_scaled)\n",
    "    sil_score = silhouette_score(X_train_scaled, labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('K-Means Elbow Method', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best K-Means with optimal k\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters (by silhouette): {best_k}\")\n",
    "\n",
    "kmeans_optimal = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "kmeans_clusters = kmeans_optimal.fit_predict(X_train_scaled)\n",
    "\n",
    "# K-Means with known number of classes\n",
    "kmeans_true_k = KMeans(n_clusters=n_classes, random_state=42, n_init=10)\n",
    "kmeans_true_clusters = kmeans_true_k.fit_predict(X_train_scaled)\n",
    "\n",
    "print(f\"K-Means (k={best_k}) Silhouette Score: {silhouette_score(X_train_scaled, kmeans_clusters):.4f}\")\n",
    "print(f\"K-Means (k={n_classes}) Silhouette Score: {silhouette_score(X_train_scaled, kmeans_true_clusters):.4f}\")\n",
    "\n",
    "# Gaussian Mixture Models\n",
    "print(f\"\\n🌀 GAUSSIAN MIXTURE MODELS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# GMM with different numbers of components\n",
    "gmm_scores = []\n",
    "gmm_aic = []\n",
    "gmm_bic = []\n",
    "\n",
    "for n_components in k_range:\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(X_train_scaled)\n",
    "    \n",
    "    labels = gmm.predict(X_train_scaled)\n",
    "    sil_score = silhouette_score(X_train_scaled, labels)\n",
    "    gmm_scores.append(sil_score)\n",
    "    gmm_aic.append(gmm.aic(X_train_scaled))\n",
    "    gmm_bic.append(gmm.bic(X_train_scaled))\n",
    "\n",
    "# Best GMM\n",
    "best_gmm_idx = np.argmax(gmm_scores)\n",
    "best_n_components = k_range[best_gmm_idx]\n",
    "\n",
    "gmm_optimal = GaussianMixture(n_components=best_n_components, random_state=42)\n",
    "gmm_clusters = gmm_optimal.fit_predict(X_train_scaled)\n",
    "\n",
    "gmm_true_k = GaussianMixture(n_components=n_classes, random_state=42)\n",
    "gmm_true_clusters = gmm_true_k.fit_predict(X_train_scaled)\n",
    "\n",
    "print(f\"Optimal number of components: {best_n_components}\")\n",
    "print(f\"GMM (n={best_n_components}) Silhouette Score: {gmm_scores[best_gmm_idx]:.4f}\")\n",
    "print(f\"GMM (n={n_classes}) Silhouette Score: {silhouette_score(X_train_scaled, gmm_true_clusters):.4f}\")\n",
    "\n",
    "# Hierarchical Clustering\n",
    "print(f\"\\n🌳 HIERARCHICAL CLUSTERING\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Compute hierarchical clustering\n",
    "linkage_matrix = linkage(X_train_scaled[:500], method='ward')  # Use subset for efficiency\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sample Index or (Cluster Size)')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Agglomerative clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_classes)\n",
    "agg_clusters = agg_clustering.fit_predict(X_train_scaled)\n",
    "\n",
    "print(f\"Hierarchical Clustering Silhouette Score: {silhouette_score(X_train_scaled, agg_clusters):.4f}\")\n",
    "\n",
    "# Clustering Results Summary\n",
    "print(f\"\\n📊 CLUSTERING RESULTS SUMMARY\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "clustering_results = {\n",
    "    'Algorithm': ['K-Means (optimal)', f'K-Means (k={n_classes})', 'GMM (optimal)', f'GMM (n={n_classes})', 'Hierarchical'],\n",
    "    'Clusters/Components': [best_k, n_classes, best_n_components, n_classes, n_classes],\n",
    "    'Silhouette Score': [\n",
    "        silhouette_score(X_train_scaled, kmeans_clusters),\n",
    "        silhouette_score(X_train_scaled, kmeans_true_clusters),\n",
    "        gmm_scores[best_gmm_idx],\n",
    "        silhouette_score(X_train_scaled, gmm_true_clusters),\n",
    "        silhouette_score(X_train_scaled, agg_clusters)\n",
    "    ]\n",
    "}\n",
    "\n",
    "clustering_df = pd.DataFrame(clustering_results)\n",
    "clustering_df = clustering_df.sort_values('Silhouette Score', ascending=False)\n",
    "print(clustering_df.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n✅ Clustering algorithms completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 3. PRINCIPAL COMPONENT ANALYSIS (PCA) ==============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           🎯 PRINCIPAL COMPONENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# PCA Analysis\n",
    "print(f\"\\n📊 PERFORMING PCA ANALYSIS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Determine optimal number of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for different variance thresholds\n",
    "var_thresholds = [0.85, 0.90, 0.95, 0.99]\n",
    "components_needed = {}\n",
    "\n",
    "for threshold in var_thresholds:\n",
    "    n_components = np.where(cumsum_var >= threshold)[0][0] + 1\n",
    "    components_needed[threshold] = n_components\n",
    "    print(f\"Components needed for {threshold*100:.0f}% variance: {n_components}\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Explained variance ratio\n",
    "axes[0].plot(range(1, min(51, len(pca_full.explained_variance_ratio_)+1)), \n",
    "             pca_full.explained_variance_ratio_[:50], 'bo-', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Explained Variance by Component', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, min(51, len(cumsum_var)+1)), \n",
    "             cumsum_var[:50], 'ro-', linewidth=2, markersize=6)\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% Variance')\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% Variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply PCA with different numbers of components\n",
    "pca_configs = [\n",
    "    ('PCA_50', 50),\n",
    "    ('PCA_95%', components_needed[0.95]),\n",
    "    ('PCA_90%', components_needed[0.90]),\n",
    "    ('PCA_85%', components_needed[0.85])\n",
    "]\n",
    "\n",
    "pca_results = {}\n",
    "\n",
    "for config_name, n_comp in pca_configs:\n",
    "    print(f\"\\n🔄 Applying {config_name} ({n_comp} components)...\")\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Test with best classifier from previous step\n",
    "    clf_copy = trained_models[best_model_name].__class__(**trained_models[best_model_name].get_params())\n",
    "    clf_copy.fit(X_train_pca, y_train)\n",
    "    y_pred_pca = clf_copy.predict(X_test_pca)\n",
    "    accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "    \n",
    "    pca_results[config_name] = {\n",
    "        'n_components': n_comp,\n",
    "        'variance_explained': variance_explained,\n",
    "        'accuracy': accuracy_pca,\n",
    "        'pca_model': pca,\n",
    "        'X_train_pca': X_train_pca,\n",
    "        'X_test_pca': X_test_pca\n",
    "    }\n",
    "    \n",
    "    print(f\"  Components: {n_comp}\")\n",
    "    print(f\"  Variance Explained: {variance_explained:.4f}\")\n",
    "    print(f\"  {best_model_name} Accuracy: {accuracy_pca:.4f}\")\n",
    "\n",
    "# PCA Results Summary\n",
    "print(f\"\\n📊 PCA RESULTS SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "pca_summary = pd.DataFrame({\n",
    "    'Configuration': list(pca_results.keys()),\n",
    "    'Components': [pca_results[config]['n_components'] for config in pca_results.keys()],\n",
    "    'Variance Explained': [pca_results[config]['variance_explained'] for config in pca_results.keys()],\n",
    "    'Accuracy': [pca_results[config]['accuracy'] for config in pca_results.keys()],\n",
    "    'Dimensionality Reduction': [f\"{(1 - pca_results[config]['n_components']/X.shape[1])*100:.1f}%\" \n",
    "                                for config in pca_results.keys()]\n",
    "})\n",
    "\n",
    "print(pca_summary.round(4).to_string(index=False))\n",
    "\n",
    "# Best PCA configuration\n",
    "best_pca_config = max(pca_results.keys(), key=lambda x: pca_results[x]['accuracy'])\n",
    "print(f\"\\n🏆 Best PCA Configuration: {best_pca_config}\")\n",
    "print(f\"   Components: {pca_results[best_pca_config]['n_components']}\")\n",
    "print(f\"   Accuracy: {pca_results[best_pca_config]['accuracy']:.4f}\")\n",
    "print(f\"   Variance Explained: {pca_results[best_pca_config]['variance_explained']:.4f}\")\n",
    "\n",
    "# Visualize PCA components (first 2 components)\n",
    "best_pca = pca_results[best_pca_config]['pca_model']\n",
    "X_train_pca_2d = best_pca.transform(X_train_scaled)[:, :2]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(label_encoder.classes_)))\n",
    "\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    mask = y_train == i\n",
    "    plt.scatter(X_train_pca_2d[mask, 0], X_train_pca_2d[mask, 1], \n",
    "               c=[colors[i]], label=class_name, alpha=0.7, s=50)\n",
    "\n",
    "plt.xlabel(f'First Principal Component (Var: {best_pca.explained_variance_ratio_[0]:.3f})')\n",
    "plt.ylabel(f'Second Principal Component (Var: {best_pca.explained_variance_ratio_[1]:.3f})')\n",
    "plt.title('PCA: First Two Principal Components', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ PCA analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 4. ENSEMBLE LEARNING METHODS ==============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           🎭 ENSEMBLE LEARNING METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Advanced Ensemble Methods\n",
    "print(f\"\\n🚀 IMPLEMENTING ENSEMBLE METHODS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Individual models for ensemble\n",
    "base_models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM_RBF': SVC(random_state=42, kernel='rbf', probability=True),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train base models\n",
    "print(\"Training base models for ensemble...\")\n",
    "for name, model in base_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"  {name}: {accuracy:.4f}\")\n",
    "\n",
    "# 1. Voting Classifier (Hard Voting)\n",
    "print(f\"\\n🗳️ VOTING CLASSIFIER\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in base_models.items()],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_hard.fit(X_train_scaled, y_train)\n",
    "y_pred_voting_hard = voting_hard.predict(X_test_scaled)\n",
    "accuracy_voting_hard = accuracy_score(y_test, y_pred_voting_hard)\n",
    "\n",
    "# 2. Voting Classifier (Soft Voting)\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in base_models.items()],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_soft.fit(X_train_scaled, y_train)\n",
    "y_pred_voting_soft = voting_soft.predict(X_test_scaled)\n",
    "accuracy_voting_soft = accuracy_score(y_test, y_pred_voting_soft)\n",
    "\n",
    "print(f\"Hard Voting Accuracy: {accuracy_voting_hard:.4f}\")\n",
    "print(f\"Soft Voting Accuracy: {accuracy_voting_soft:.4f}\")\n",
    "\n",
    "# 3. Bagging with Decision Trees\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "print(f\"\\n🎒 BAGGING CLASSIFIER\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train_scaled, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test_scaled)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "print(f\"Bagging Accuracy: {accuracy_bagging:.4f}\")\n",
    "\n",
    "# 4. AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "print(f\"\\n🚀 ADABOOST CLASSIFIER\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "adaboost.fit(X_train_scaled, y_train)\n",
    "y_pred_adaboost = adaboost.predict(X_test_scaled)\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "\n",
    "print(f\"AdaBoost Accuracy: {accuracy_adaboost:.4f}\")\n",
    "\n",
    "# 5. Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(f\"\\n🌲 EXTRA TREES CLASSIFIER\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "extra_trees = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "extra_trees.fit(X_train_scaled, y_train)\n",
    "y_pred_extra_trees = extra_trees.predict(X_test_scaled)\n",
    "accuracy_extra_trees = accuracy_score(y_test, y_pred_extra_trees)\n",
    "\n",
    "print(f\"Extra Trees Accuracy: {accuracy_extra_trees:.4f}\")\n",
    "\n",
    "# 6. Stacking Classifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "print(f\"\\n📚 STACKING CLASSIFIER\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Define base learners for stacking\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "    ('svm', SVC(random_state=42, probability=True)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner = LogisticRegression(random_state=42)\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "y_pred_stacking = stacking.predict(X_test_scaled)\n",
    "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "print(f\"Stacking Accuracy: {accuracy_stacking:.4f}\")\n",
    "\n",
    "# Ensemble Results Summary\n",
    "print(f\"\\n📊 ENSEMBLE METHODS COMPARISON\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "ensemble_results = {\n",
    "    'Method': [\n",
    "        'Voting (Hard)', 'Voting (Soft)', 'Bagging', \n",
    "        'AdaBoost', 'Extra Trees', 'Stacking',\n",
    "        'Best Individual'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        accuracy_voting_hard, accuracy_voting_soft, accuracy_bagging,\n",
    "        accuracy_adaboost, accuracy_extra_trees, accuracy_stacking,\n",
    "        max([results[model]['accuracy'] for model in results.keys()])\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Majority vote (hard)', 'Probability vote (soft)', 'Bootstrap aggregating',\n",
    "        'Adaptive boosting', 'Extremely randomized trees', 'Meta-learning',\n",
    "        f'Best single model ({best_model_name})'\n",
    "    ]\n",
    "}\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_results)\n",
    "ensemble_df = ensemble_df.sort_values('Accuracy', ascending=False)\n",
    "print(ensemble_df.round(4).to_string(index=False))\n",
    "\n",
    "# Best ensemble method\n",
    "best_ensemble_idx = ensemble_df.index[0]\n",
    "best_ensemble_name = ensemble_df.iloc[0]['Method']\n",
    "best_ensemble_accuracy = ensemble_df.iloc[0]['Accuracy']\n",
    "\n",
    "print(f\"\\n🏆 Best Ensemble Method: {best_ensemble_name}\")\n",
    "print(f\"   Accuracy: {best_ensemble_accuracy:.4f}\")\n",
    "\n",
    "# Feature importance from Random Forest (one of the best performers)\n",
    "if 'RandomForest' in trained_models:\n",
    "    rf_model = trained_models['Random Forest']\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # Get top 20 important features\n",
    "    feature_names = X.columns\n",
    "    top_features_idx = np.argsort(feature_importance)[-20:]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(20), feature_importance[top_features_idx])\n",
    "    plt.yticks(range(20), [feature_names[i] for i in top_features_idx])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 20 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Ensemble learning methods completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 5. HIDDEN MARKOV MODEL (HMM) ==============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           🔄 HIDDEN MARKOV MODEL\")\n",
    "print(\"        SEQUENTIAL POSE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install hmmlearn if not available\n",
    "try:\n",
    "    from hmmlearn import hmm\n",
    "except ImportError:\n",
    "    print(\"Installing hmmlearn...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'hmmlearn'])\n",
    "    from hmmlearn import hmm\n",
    "\n",
    "print(f\"\\n📊 PREPARING SEQUENTIAL DATA FOR HMM\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sequences for HMM\n",
    "# Group data by pose_id to create sequences (simulating temporal data)\n",
    "def create_sequences(X, y, sequence_length=5):\n",
    "    \"\"\"Create sequences for HMM training\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Sort by pose_id to simulate temporal ordering\n",
    "    sorted_indices = np.argsort(merged_df['pose_id'].values)\n",
    "    X_sorted = X.iloc[sorted_indices]\n",
    "    y_sorted = y[sorted_indices]\n",
    "    \n",
    "    # Create overlapping sequences\n",
    "    for i in range(0, len(X_sorted) - sequence_length + 1, sequence_length//2):\n",
    "        seq = X_sorted.iloc[i:i+sequence_length].values\n",
    "        seq_labels = y_sorted[i:i+sequence_length]\n",
    "        \n",
    "        # Only include sequences with consistent labels (for simplicity)\n",
    "        if len(np.unique(seq_labels)) == 1:\n",
    "            sequences.append(seq)\n",
    "            labels.append(seq_labels[0])\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 5\n",
    "X_sequences, y_sequences = create_sequences(X, y_encoded, sequence_length)\n",
    "\n",
    "print(f\"Created {len(X_sequences)} sequences of length {sequence_length}\")\n",
    "print(f\"Sequence shape: {X_sequences.shape}\")\n",
    "\n",
    "# Split sequences into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences\n",
    ")\n",
    "\n",
    "print(f\"Train sequences: {len(X_seq_train)}\")\n",
    "print(f\"Test sequences: {len(X_seq_test)}\")\n",
    "\n",
    "# Train HMM for each pose class\n",
    "print(f\"\\n🔄 TRAINING HMM MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "hmm_models = {}\n",
    "n_components = 3  # Number of hidden states per pose\n",
    "\n",
    "for class_idx, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"Training HMM for {class_name}...\")\n",
    "    \n",
    "    # Get sequences for this class\n",
    "    class_sequences = X_seq_train[y_seq_train == class_idx]\n",
    "    \n",
    "    if len(class_sequences) > 0:\n",
    "        # Flatten sequences for training\n",
    "        X_class = np.vstack(class_sequences)\n",
    "        \n",
    "        # Create sequence lengths array\n",
    "        lengths = [sequence_length] * len(class_sequences)\n",
    "        \n",
    "        # Train Gaussian HMM\n",
    "        model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", random_state=42)\n",
    "        \n",
    "        try:\n",
    "            model.fit(X_class, lengths)\n",
    "            hmm_models[class_idx] = model\n",
    "            print(f\"  ✓ {class_name}: {len(class_sequences)} sequences\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {class_name}: Failed to train ({str(e)})\")\n",
    "    else:\n",
    "        print(f\"  ✗ {class_name}: No sequences available\")\n",
    "\n",
    "print(f\"\\nSuccessfully trained {len(hmm_models)} HMM models\")\n",
    "\n",
    "# HMM Prediction Function\n",
    "def predict_hmm(sequence, models, label_encoder):\n",
    "    \"\"\"Predict pose using HMM models\"\"\"\n",
    "    if len(models) == 0:\n",
    "        return -1, 0.0\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_class = -1\n",
    "    \n",
    "    for class_idx, model in models.items():\n",
    "        try:\n",
    "            score = model.score(sequence)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_class = class_idx\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return best_class, best_score\n",
    "\n",
    "# Test HMM models\n",
    "print(f\"\\n📈 TESTING HMM MODELS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "hmm_predictions = []\n",
    "hmm_scores = []\n",
    "\n",
    "for i, test_seq in enumerate(X_seq_test):\n",
    "    pred_class, score = predict_hmm(test_seq, hmm_models, label_encoder)\n",
    "    hmm_predictions.append(pred_class)\n",
    "    hmm_scores.append(score)\n",
    "\n",
    "# Calculate accuracy\n",
    "valid_predictions = [i for i, pred in enumerate(hmm_predictions) if pred != -1]\n",
    "hmm_predictions_valid = [hmm_predictions[i] for i in valid_predictions]\n",
    "y_test_valid = [y_seq_test[i] for i in valid_predictions]\n",
    "\n",
    "if len(hmm_predictions_valid) > 0:\n",
    "    hmm_accuracy = accuracy_score(y_test_valid, hmm_predictions_valid)\n",
    "    print(f\"HMM Accuracy: {hmm_accuracy:.4f}\")\n",
    "    print(f\"Valid predictions: {len(valid_predictions)}/{len(X_seq_test)}\")\n",
    "    \n",
    "    # Classification report\n",
    "    if len(set(hmm_predictions_valid)) > 1:\n",
    "        print(f\"\\nHMM Classification Report:\")\n",
    "        print(classification_report(y_test_valid, hmm_predictions_valid, \n",
    "                                  target_names=[label_encoder.classes_[i] for i in sorted(set(hmm_predictions_valid))]))\n",
    "else:\n",
    "    print(\"No valid predictions from HMM models\")\n",
    "\n",
    "# Transition Matrices Visualization\n",
    "if len(hmm_models) > 0:\n",
    "    print(f\"\\n📊 HMM TRANSITION MATRICES\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Plot transition matrices for first few models\n",
    "    fig, axes = plt.subplots(1, min(3, len(hmm_models)), figsize=(15, 5))\n",
    "    if len(hmm_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (class_idx, model) in enumerate(list(hmm_models.items())[:3]):\n",
    "        class_name = label_encoder.classes_[class_idx]\n",
    "        \n",
    "        if len(hmm_models) > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes[0]\n",
    "            \n",
    "        sns.heatmap(model.transmat_, annot=True, cmap='Blues', ax=ax, \n",
    "                   fmt='.3f', cbar_kws={'label': 'Transition Probability'})\n",
    "        ax.set_title(f'Transition Matrix - {class_name}', fontweight='bold')\n",
    "        ax.set_xlabel('To State')\n",
    "        ax.set_ylabel('From State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare HMM with other methods\n",
    "print(f\"\\n📊 SEQUENTIAL vs STATIC PREDICTION COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test static classifier on flattened sequences\n",
    "X_seq_test_flat = X_seq_test.reshape(X_seq_test.shape[0], -1)\n",
    "X_seq_train_flat = X_seq_train.reshape(X_seq_train.shape[0], -1)\n",
    "\n",
    "# Scale the flattened sequences\n",
    "scaler_seq = StandardScaler()\n",
    "X_seq_train_flat_scaled = scaler_seq.fit_transform(X_seq_train_flat)\n",
    "X_seq_test_flat_scaled = scaler_seq.transform(X_seq_test_flat)\n",
    "\n",
    "# Train a static classifier on sequences\n",
    "static_seq_clf = trained_models[best_model_name].__class__(**trained_models[best_model_name].get_params())\n",
    "static_seq_clf.fit(X_seq_train_flat_scaled, y_seq_train)\n",
    "static_seq_pred = static_seq_clf.predict(X_seq_test_flat_scaled)\n",
    "static_seq_accuracy = accuracy_score(y_seq_test, static_seq_pred)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['HMM (Sequential)', f'{best_model_name} (Static on Sequences)', f'{best_model_name} (Single Frames)'],\n",
    "    'Accuracy': [\n",
    "        hmm_accuracy if 'hmm_accuracy' in locals() else 0.0,\n",
    "        static_seq_accuracy,\n",
    "        results[best_model_name]['accuracy']\n",
    "    ],\n",
    "    'Data Type': ['Sequential', 'Flattened Sequences', 'Single Frames']\n",
    "})\n",
    "\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "print(\"✅ HMM sequential modeling completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
